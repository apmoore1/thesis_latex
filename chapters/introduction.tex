\section{Introduction}
Sentiment analysis is a large sub field within the larger research area of Natural Language Processing (NLP), it has been defined by \citet{liu2015sentiment} as the analysis of people's opinions towards entities in written text. This definition to some degree is slightly out of date as the field has grown to encompass more information than just written text, such as images, voice, and video\footnote{The use of more than one source of information e.g. voice and text, is commonly known as multimodal.} \citep{poria2016fusing}. However as this thesis only uses textual information, this definition is representative of sentiment analysis in the research presented here. Following this definition, people's opinions are represented by sentiment values such as positive, negative, and neutral. Further, the entities within the definition can be an organisation, person, object, or more broadly a concept or topic such as politics within Britain. 


The applications and thus the motivation for sentiment analysis are many and varied, due to a large degree the massive quantities of unstructured textual data that the web contains. Application examples include understanding what audiences liked and disliked within films and trailers \citep{pereg-etal-2019-absapp}, linking the sentiment of the text about\langcorrections{changed} a film over time with a\langcorrections{added} film's success \citep{Vecchio2018TheDS}, and providing additional information for analysis on political tweets \citep{wang-etal-2017-totemss}. Further, many businesses sell sentiment analysis via an application programming interface (API) including Google\footnote{\url{https://cloud.google.com/natural-language}}, Microsoft\footnote{\url{https://azure.microsoft.com/en-gb/services/cognitive-services/text-analytics/}}, and Aylien\footnote{\url{https://aylien.com/text-analysis-platform/}}. This thesis is less focussed in the end application of sentiment analysis, but rather whether the empirical evaluation of the methods created for the most fine grained version, target dependent sentiment analysis (TDSA), can be improved.

%what do these increases in results mean?
The research area of TDSA has recently seen a surge\footnote{The chart from Papers With Code, shown here \url{https://paperswithcode.com/sota/aspect-based-sentiment-analysis-on-semeval}, plots a paper's method and its metric score on a set of popular TDSA datasets over time. Note that the chart states it is for the task Aspect-Based Sentiment Analysis, but in this thesis the task is named TDSA, the difference between the two will be explained in chapter \ref{chapter:lit_review}.} of new research most likely due to commercial applications. This is fantastic for any area of research, however current research, in the majority of cases, is only publishing positive results\footnote{This is in reference to negative publication bias, of which there is a NLP workshop at EMNLP 2020 that focuses on negative results \url{https://insights-workshop.github.io/}.} on very similar datasets without any empirical or quantitative reasons for these positive results. Thus TDSA methods cannot be shown empirically to be generalisable, where in this thesis generalisable means that a method performs well\footnote{In this thesis a method performs `well' when it performs either significantly better than all other methods or it is not significantly different to the other top performing methods.} across many vastly different datasets. Further, evaluation within this generalisable setup is approached by training and evaluating a method on each dataset independently. By testing methods for generalisability, rather than on a few similar datasets, it is possible to find out whether one method performs well across the board in all circumstances, or the more likely case of which methods perform best for different dataset properties. Thus testing for generalisability would allow researchers to get a better understanding of their method, where the method can be improved, and based on the dataset properties if it is the best method to use.

%However this type of analysis is rather coarse grained and requires comparison at the dataset level.

Even though generalisability can show some reasons why a method may work well on one dataset compared to another, e.g. a\langcorrections{Added} method performs well on review data and not social media, \langcorrections{Removed}this form of analysis cannot explain why one method is better than another within a dataset. Thus a more fine grained analysis that can test different phenomena within a dataset is required, especially as the difference in results between methods on some dataset are marginal. Some approaches for fine grained analysis already exist that can test different phenomena within TDSA through error splits\footnote{We define an error split as a method of splitting an existing dataset into subsets of data, whereby each subset satisfies a different condition of the split. For instance the error split suggested by \citet{wang-etal-2017-tdparse} is conditioned on the number of unique sentiments in a text, in this case for datasets with two unique sentiment values there would be only two subsets of data, the first subset for texts with a single unique sentiment, and the second for texts with two unique sentiments.} of a dataset. Within the existing literature, several different error splits have been suggested that probe different phenomena within TDSA \citep{nguyen-shirai-2015-phrasernn,wang-etal-2017-tdparse,he-etal-2018-exploiting,yang2018multi}. However,\langcorrections{Added} so far no work has performed a detailed analysis of these error splits, nor has any work examined how these error splits can be improved upon. By better understanding what these prior works' error splits probe, improvements could be made, and more rigorous evaluation methodology can be created for TDSA.

If the results from a paper are difficult or impossible to reproduce or replicate\footnote{The definition of the differences between reproduce and replicate will be explained in chapter \ref{chapter:reproducibility}.} to a large extent that research is pointless, and this has been expressed through the fictional tale of the Zigglebottom tagger \citep{pedersen-2008-last}. From a review of the literature it has been found that one particular Neural Network (NN) based TDSA method has been difficult to reproduce or replicate\footnote{See chapter \ref{chapter:reproducibility} section \ref{section:repro_lstm}.}\langcorrections{Space removed between footnote and last character.}. Within neural sequence labelling,\langcorrections{Added.} \citet{reimers-gurevych-2017-reporting} \langcorrections{Removed}found that NN methods can produce significantly different results between multiple runs due to random initialisations. Thus as these prior NN works within TDSA have only reported single runs it is plausible that the reason for the lack of replication or reproducibility could be due to not reporting multiple runs. Therefore a review within reporting standards for NN TDSA approaches is required for both reproducibility and fair evaluation.

The main goal of this thesis is to improve evaluation and reporting within TDSA so that researchers can better understand and reproduce the methods they create. This thesis also goes one step further whereby instead of just theoretically and empirically showing how evaluation and reporting can be improved, the whole thesis is `executable' as in all code to produce the findings within the thesis is reproducible through Jupyter notebooks\footnote{\url{https://jupyter.org/}.} and related codebases. All codebases and notebooks that are relevant to different sections to produce the results within those sections are stated within footnotes as URLs attached to the relevant section headers. Lastly all empirical results within this thesis are on datasets within the English language\footnote{This is highlighted following what has been known as the \#BenderRule \citep{bender2019rule}, which re-iterated a point made in prior work \citep{bender2011achieving}, that not stating the language (normally English) the data has come from misleads the reader into thinking the work is language independent.}.


\section{Research Questions}
Building on the motivation provided above, these are the research questions that the thesis will answer:

\begin{researchq}
What lessons can be learned from reproducing a method within TDSA? 
\label{rq:lessons}
\end{researchq}

\begin{researchq}
How generalisable are existing methods within TDSA?
\label{rq:generalisable}
\end{researchq}

\begin{researchq}
What is an appropriate empirical evaluation methodology for TDSA?
\label{rq:measured}
\end{researchq}

\section{Contributions and Findings}
\begin{itemize}
    \item \textbf{The creation of a new definition, the hextuple, for fine grained sentiment analysis that directly extends the current definition by \citet{liu2015sentiment}.}\newline
    The extended fine grained sentiment analysis definition in comparison to the existing by \citet{liu2015sentiment} removes any ambiguity that arises from the context\footnote{Ambiguity is later defined, within section \ref{section:lit_review_fine_grained_sentiment_analysis_intro}, as sentiment ambiguity.}. This ambiguity is motivated through multiple examples and an empirical analysis of existing datasets, wherein this empirical analysis found that for two datasets 3.68\% and 3.27\% of samples would be impossible to classify through Liu's \citep{liu2015sentiment}\langcorrections{Add the 's} original definition, due to ambiguity. Further by analysing the existing definition by \citet{liu2015sentiment}, this thesis justifies parts of the definition through ambiguity, whereas in comparison the original justifications from \citet{liu2015sentiment} were motivated through application rather than a more theoretical ambiguity perspective.  
    \item \textbf{The first reproduction study within TDSA.}\newline
    Through the reproduction studies of two non-NN based methods \citep{vo2015target, wang-etal-2017-tdparse}, neither report a factor that is significant in reproducing their results, further both of these factors, scaling features and the C-value of a Support Vector Machine (SVM) \citep{chang2011libsvm}, are found across many datasets to be significant for both methods. Additionally it is found for NN based methods within TDSA that multiple runs can produce significantly different results, which is what \citet{reimers-gurevych-2017-reporting} found for neural sequence labelling methods. The distribution of results created from the NN method through multiple runs is thus believed to be the reason why prior works found it difficult to reproduce or replicate a particular NN TDSA method \citep{tang-etal-2016-effective}.
    \item \textbf{The largest empirical evaluation of TDSA methods.}\newline
    Due to reproducing diverse methods, in this case non-NN and NN methods, and applying them to a large range of different datasets, it was possible to test for generalisability. Finding that no one method was best or generalisable, but factors such as dataset size and sentiment class distribution determined whether it was best to use a NN or non-NN method. Further it was found for the NN methods that by evaluating on a larger and more diverse set of datasets than the original authors used, the novel NN methods that were created on some dataset were found to be no better on average than the original baseline method.
    \item \textbf{The creation of a new empirical evaluation methodology for TDSA.}\newline
    Prior works' error splits are tested and formalised with the addition of new error splits and metrics created within this thesis. Through an extensive empirical evaluation, two of the existing error splits and one of the new error splits are not recommended to be used, due to them not measuring what they were hypothesised to measure. From the recommended error splits and metrics, one error split and the metrics have been created within this thesis to test for new phenomena within TDSA. These recommended error splits and metrics make up this new empirical evaluation methodology for TDSA. The new empirical evaluation methodology is then tested on multiple case studies whereas the prior work had only justified their hypothesis through small qualitative examples or using an error split that this analysis does not recommend. The case studies demonstrated the use of the empirical evaluation methodology. These analyses and experiments when combined create the largest and most extensive review of error analysis within TDSA to date.
    \item \textbf{An executable thesis.}\newline
    As stated earlier this entire thesis is reproducible through the codebases and Jupyter notebooks that are attached to the relevant sections throughout the thesis. All the results that have been generated have used either the Bella\footnote{\url{https://github.com/apmoore1/Bella}} or target-extraction\footnote{\url{https://github.com/apmoore1/target-extraction}} packages created during the course of the PhD. The Bella package is more focused towards the non-NN methods whereas target-extraction is only focused on the NN methods. Both packages have extensive unit tests and Bella has an easy to use out of the box functionality through its model zoo.
\end{itemize}

\section{Organisation of the Thesis}
\begin{itemize}
    \item \textbf{Chapter \ref{chapter:lit_review}: Literature Review.}\newline
    The majority of the literature review summarises the different levels of sentiment analysis starting with the most coarse grained (document level) and finishing with fine grained sentiment analysis. The review of coarse to fine grained sentiment analysis is one of, if not, the most extensive review of sentiment analysis, within the English language, showcasing the different granularities and how they link together. Within the fine grained sentiment analysis review a new extended definition for fine grained sentiment analysis is stated. Further, the literature on TDSA has been reviewed from which it states the need for and lack of a reproducibility study. Further it motivates that error splits that do exist have not been rigorously analysed to better understand what they show and when they are useful. The review finishes with some further related topics to fine grained sentiment analysis, which motivate some of the future work within the conclusion chapter.
    \item \textbf{Chapter \ref{chapter:reproducibility}: Reproducibility and Generalisability of TDSA Methods.}\newline
    It details within the introduction the motivations behind performing both the reproducibility studies and generalisation experiments. The chapter provides a more extensive related work section detailing prior work within reproducibility more broadly and then concentrating on the most related work within sentiment analysis on both reproducibility and generalisability. The methods used in both the reproducibility and generalisability are described in detail. The reproduction studies are conducted, and the results are used to answer \rqref{rq:lessons}. Lastly the generalisation experiments are performed whereby the results allow \rqref{rq:generalisable} to be answered.
    \item \textbf{Chapter \ref{chapter:methodology}: Improving Experimental Methodology for TDSA.}\newline
    This chapter contains an extensive review comparing and contrasting previous error splits within TDSA. Two new error splits are created to measure different phenomena, compared to the existing splits. All error splits are then reviewed on three English datasets, showcasing the difference between the datasets using the error splits. These error splits are then summarised describing what they do and what they hypothetically measure. The error splits are then tested across several TDSA methods and three English datasets to conclude if the error splits are measuring what was hypothesised. From these experiments, a new TDSA metric and its variants are created. The detailed review, analysis, and tests of the error splits in conjunction with the new metric allows for \rqref{rq:measured} to be answered.
    \item \textbf{Chapter \ref{chapter:case_study_methodology}: Case Studies in Improving Experimental Methodology for TDSA.}\newline
    The new experimental methodology for TDSA created within chapter \ref{chapter:methodology} is then explored through several case studies. Each case study explores a new development whereby these new development including position encoding, inter-aspect encoding, and transfer learning from a language model\footnote{Also known as contextualised word representations.}. Each development is then applied to the methods used within chapter \ref{chapter:methodology}, when appropriate, and evaluated using the new experimental methodology. Within each case study the findings from the new experimental methodology are reviewed and where appropriate will be compared to the hypothesis that motivated the relevant development.   
    \item \textbf{Chapter \ref{chapter:conclusion}: Conclusion.}\newline
    The conclusion summarises\langcorrections{Corrected} the thesis, revisits the research questions, and finishes with future work.
\end{itemize}