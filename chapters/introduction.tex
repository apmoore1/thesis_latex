\section{Introduction}
Sentiment analysis is a large sub field within the larger research area of Natural Language Processing (NLP), it has been defined by \citet{liu2015sentiment} as the analysis of people's opinions towards entities in written text. This definition to some degree is slightly out of date as the field has grown to encompass more information than just written text, such as images, voice, and video\footnote{The use of more than one source of information e.g. voice and text, is commonly known as multimodal.} \citep{poria2016fusing}. However as this thesis only uses textual information, this definition is representative of sentiment analysis in the research presented here. Following this definition, people's opinions are represented by sentiment values such as positive, negative, and neutral. Further, the entities within the definition can be an organisation, person, object, or more broadly a concept or topic such as politics within Britain. 


The applications and thus the motivation for sentiment analysis are many and varied, due to a large degree the massive quantities of unstructured textual data that the web contains. Application examples include understanding what audiences liked and disliked within films and trailers \citep{pereg-etal-2019-absapp}, linking the sentiment of the text of a film over time with film's success \citep{Vecchio2018TheDS}, and providing additional information for analysis on political tweets \citep{wang-etal-2017-totemss}. Further, many businesses sell sentiment analysis via an application programming interface (API) including Google\footnote{\url{https://cloud.google.com/natural-language}}, Microsoft\footnote{\url{https://azure.microsoft.com/en-gb/services/cognitive-services/text-analytics/}}, and Aylien\footnote{\url{https://aylien.com/text-analysis-platform/}}. This thesis is less focussed in the end application of sentiment analysis, but rather whether the empirical evaluation of the methods created for the most fine grained version, target dependent sentiment analysis (TDSA), can be improved.

%what do these increases in results mean?
The research area of TDSA has recently seen a surge\footnote{The chart from Papers With Code, shown here \url{https://paperswithcode.com/sota/aspect-based-sentiment-analysis-on-semeval}, plots a paper's method and its metric score on a set of popular TDSA datasets over time. Note that the chart states it is for the task Aspect-Based Sentiment Analysis, but in this thesis the task is named TDSA, the difference between the two will be explained in chapter \ref{chapter:lit_review}.} of new research most likely due to commercial applications. This is fantastic for any area of research, however current research, in the majority of cases, is only publishing positive results\footnote{This is in reference to negative publication bias, of which there is a NLP workshop at EMNLP 2020 that focuses on negative results \url{https://insights-workshop.github.io/}.} on very similar datasets without any empirical or quantitative reasons for these positive results. Thus TDSA methods cannot be shown empirically to be generalisable, where in this thesis generalisable means that a method performs well\footnote{In this thesis a method performs `well' when it performs either significantly better than all other methods or it is not significantly different to the other top performing methods.} across many vastly different datasets. Further, evaluation within this generalisable setup is approached by training and evaluating a method on each dataset independently. By testing methods for generalisability, rather than on a few similar datasets, it is possible to find out whether one method performs well across the board in all circumstances, or the more likely case of which methods perform best for different dataset properties. Thus testing for generalisability would allow researchers to get a better understanding of their method, where the method can be improved, and based on the dataset properties if it is the best method to use.

%However this type of analysis is rather coarse grained and requires comparison at the dataset level.

Even though generalisability can show some reasons why a method may work well on one dataset compared to another e.g. method performs well on review data and not social media, it is the case this form of analysis cannot explain why one method is better than another within a dataset. Thus a more fine grained analysis that can test different phenomena within a dataset is required, especially as the difference in results between methods on some dataset are marginal. Some approaches for fine grained analysis already exist that can test different phenomena within TDSA through error splits of a dataset. Within the existing literature, several different error splits have been suggested that probe different phenomena within TDSA \citep{nguyen-shirai-2015-phrasernn,wang-etal-2017-tdparse,he-etal-2018-exploiting,yang2018multi}. However so far no work has performed a detailed analysis of these error splits, nor has any work examined how these error splits can be improved upon. By better understanding what these prior works' error splits probe, improvements could be made, and more rigorous evaluation methodology can be created for TDSA.

If the results from a paper are difficult or impossible to reproduce or replicate\footnote{The definition of the differences between reproduce and replicate will be explained in chapter \ref{chapter:reproducibility}.} to a large extent that research is pointless, and this has been expressed through the fictional tale of the Zigglebottom tagger \citep{pedersen-2008-last}. From a review of the literature it has been found that one particular Neural Network (NN) based TDSA method has been difficult to reproduce or replicate \footnote{See chapter \ref{chapter:reproducibility} section \ref{section:repro_lstm}.}. Within neural sequence labelling \citet{reimers-gurevych-2017-reporting} has found that NN methods can produce significantly different results between multiple runs due to random initialisations. Thus as these prior NN works within TDSA have only reported single runs it is plausible that the reason for the lack of replication or reproducibility could be due to not reporting multiple runs. Therefore a review within reporting standards for NN TDSA approaches is required for both reproducibility and fair evaluation.

The main goal of this thesis is to improve evaluation and reporting within TDSA so that researchers can better understand and reproduce the methods they create. This thesis also goes one step further whereby instead of just theoretically and empirically showing how evaluation and reporting can be improved, the whole thesis is `executable' as in all code to produce the findings within the thesis is reproducible through Jupyter notebooks\footnote{\url{https://jupyter.org/}.} and related codebases. All codebases and notebooks that are relevant to different sections to produce the results within those sections are stated within footnotes as URLs attached to the relevant section headers. Lastly all empirical results within this thesis are on datasets within the English language.

%that this type of analysis is rather coarse grained and can only test certain phenomena. A more fine grained analysis would be of use to test phenomena within TDSA that cannot be done at a dataset level, such as testing if a method can only predict the most frequent sentiment within a text when more     would be creating either a new dataset level metric or error ananlysis splits 

%An alternative approach would be an analysis within a dataset that can given some quantitative justification to the phenomena that a method is normally trying to solve or improve upon. Some works have justified their methods through qualitative analysis \citep{hazarika-etal-2018-modeling}, rather than quantitative. This qualitative analysis normally entails a few examples that contain the phenomena that their method is improving upon, whereby normally they show their method to correctly predict the desired result, whereas the prior methods could not. These small qualitative analyses are useful as examples, but cannot be used to justify why there is an improvement, unless the qualitative analysis is detailed in explaining how the samples were chosen, or else this analysis is not reproducible and can be seen as choosing favourable samples\footnote{Also known as cherry picking.}. From this it is clear that a quantitative approach to justify different phenomena within TDSA is desirable.


%Most papers within TDSA that do have some form of qualitative analysis do not perform this detailed reproducible analysis, most of the time they provide several, what would appear favourable, samples.

%A method is generalisable within this thesis if it performs well across many datasets whereby the datasets are vastly different. In this thesis the concept of well is based upon the difference in performance to other methods. For example if method A performs the best on all datasets, method C performs only 1\% worse than A on all, and method B performs 50\% worse than A on all then method A and C generally perform well but B does not. Lastly a much stricter approach to evaluating a methods generalisability is by only training the method once and evaluating on multiple datasets that could come from very different distribution. In this thesis a less strict approach is taken whereby the method is trained and evaluated on each dataset independently. This less strict approach can still be used to test if a method has been specially created for a certain data distribution e.g. social media data or if the method generalises to other data distributions. 


%Some of these improvements may be explained/justified through a small qualitative analysis, which normally entails a few examples that contain some linguistic phenomena, whereby the new method correctly predicts the desired result whereas prior methods could not. These small qualitative analysis are great as examples, but cannot be used to justify why there is an improvement, unless the qualitative analysis is detailed in explaining how the samples were chosen or else this analysis is not reproducible and can/will be seen as choosing favourable samples\footnote{Also known as cherry picking.}. Most papers within TDSA that do have some form of qualitative analysis do not perform this detailed reproducible analysis, most of the time they provide several, what would appear favourable, samples. Furthermore through exploring the literature\footnote{See chapter \ref{chapter:reproducibility} section \ref{section:repro_lstm}.} it was found that a couple of papers could not reproduce nor replicate\footnote{Definition of the differences between reproduce and replicate will be explained in chapter \ref{chapter:reproducibility}.} the results of another. If papers cannot be reproduced or replicated to a large extent that research within the paper is pointless, which has been expressed through the fictional tale of the Zigglebottom tagger \citep{pedersen-2008-last}. This thesis explores these evaluative and reproducible problems within TDSA.

%Furthermore if the results from any of these papers are difficult or impossible to reproduce or replicate\footnote{Definition of the differences between reproduce and replicate will be explained in chapter \ref{chapter:reproducibility}.} to a large extent that research is pointless, which has been expressed through the fictional tale of the Zigglebottom tagger \citep{pedersen-2008-last}.



%This thesis tackles these evaluative problems within TDSA. Through exploring the literature\footnote{See chapter \ref{chapter:reproducibility} section \ref{section:repro_lstm}.} it was found that a couple of papers could not reproduce nor replicate the results of another, of which this brings into question, why is their a difference in results? Through this one particular study of a Neural Network (NN) method it was found, for the first time in TDSA, that these NN based methods differ significantly between runs, of which this has been found before for neural sequence labelling methods \citep{reimers-gurevych-2017-reporting}. Thus this reproduction study discovered that without sufficient reporting of multiple runs of these methods it would be difficult to know if the method has been reproduced. Further it shows that without reporting the distribution of these runs the method cannot be fairly evaluated due to the significant difference between run results for the same method. Therefore in this thesis it is recommended to follow the advice given by \citet{reimers-gurevych-2017-reporting} to report multiple run results for NN based methods for both the reproducibility and evaluative reasons given. Additionally two other non-NN based methods were reproduced, finding, for the first time, that for both papers they do not report either scaling features or the value of a hyperparameter\footnote{The hyperparameter is the C-value for a Support Vector Machine (SVM) \citep{chang2011libsvm}.} that are both found to be significant in reproducing the methods. These reproduction studies highlight factors that are not reported within the original papers that cause significant difference in results 







%Thus, evaluating a method only on a subset of existing datasets limits the conclusions that can be drawn from those evaluated methods as they might perform well on those evaluated datasets, but it is unknown if they perform well on datasets from a different medium, domain, etc.

\section{Research Questions}
Building on the motivation provided above, these are the research questions that the thesis will answer:

%The main research goal of this thesis revolves around evaluation and reporting within TDSA. It is shown throughout the thesis that evaluative rigour is generally poor and to a large extent this is not known within the community. Further the community lacks detailed reporting of the methods used, which hinders reproducability. To better d  

% How can TDSA methods be better empirically evaluated?

% can phrase as an overall aim.

\begin{researchq}
What lessons can be learned from reproducing a method within TDSA? 
\label{rq:lessons}
\end{researchq}

\begin{researchq}
How generalisable are existing methods within TDSA?
\label{rq:generalisable}
\end{researchq}

\begin{researchq}
What is an appropriate empirical evaluation methodology for TDSA?
\label{rq:measured}
\end{researchq}

\section{Contributions and Findings}
\begin{itemize}
    \item \textbf{The creation of a new definition, the hextuple, for fine grained sentiment analysis that directly extends the current definition by \citet{liu2015sentiment}.}\newline
    The extended fine grained sentiment analysis definition in comparison to the existing by \citet{liu2015sentiment} removes any ambiguity that arises from the context\footnote{Ambiguity is later defined, within section \ref{section:lit_review_fine_grained_sentiment_analysis_intro}, as sentiment ambiguity.}. This ambiguity is motivated through multiple examples and an empirical analysis of existing datasets, wherein this empirical analysis found that for two datasets 3.68\% and 3.27\% of samples would be impossible to classify through \citet{liu2015sentiment} original definition, due to ambiguity. Further by analysing the existing definition by \citet{liu2015sentiment}, this thesis justifies parts of the definition through ambiguity, whereas in comparison the original justifications from \citet{liu2015sentiment} were motivated through application rather than a more theoretical ambiguity perspective.  
    \item \textbf{The first reproduction study within TDSA.}\newline
    Through the reproduction studies of two non-NN based methods \citep{vo2015target, wang-etal-2017-tdparse}, neither report a factor that is significant in reproducing their results, further both of these factors, scaling features and the C-value of a Support Vector Machine (SVM) \citep{chang2011libsvm}, are found across many datasets to be significant for both methods. Additionally it is found for NN based methods within TDSA that multiple runs can produce significantly different results, which is what \citet{reimers-gurevych-2017-reporting} found for neural sequence labelling methods. The distribution of results created from the NN method through multiple runs is thus believed to be the reason why prior works found it difficult to reproduce or replicate a particular NN TDSA method.
    \item \textbf{The largest empirical evaluation of TDSA methods.}\newline
    Due to reproducing diverse methods, in this case non-NN and NN methods, and applying them to a large range of different datasets, it was possible to test for generalisability. Finding that no one method was best or generalisable, but factors such as dataset size and sentiment class distribution determined whether it was best to use a NN or non-NN method. Further it was found for the NN methods that by evaluating on a larger and more diverse set of datasets than the original authors used, the novel NN methods that were created on some dataset were found to be no better on average than the original baseline method.
    \item \textbf{The creation of a new empirical evaluation methodology for TDSA.}\newline
    Prior works' error splits are tested and formalised with the addition of new error splits and metrics created within this thesis. Through an extensive empirical evaluation, two of the existing error splits and one of the new error splits are not recommended to be used, due to them not measuring what they were hypothesised to measure. From the recommended error splits and metrics, one error split and the metrics have been created within this thesis to test for new phenomena within TDSA. These recommended error splits and metrics make up this new empirical evaluation methodology for TDSA. The new empirical evaluation methodology is then tested on multiple case studies whereas the prior work had only justified their hypothesis through small qualitative examples or using an error split that this analysis does not recommend. The case studies demonstrated the use of the empirical evaluation methodology. These analyses and experiments when combined create the largest and most extensive review of error analysis within TDSA to date.
    \item \textbf{An executable thesis.}\newline
    As stated earlier this entire thesis is reproducible through the codebases and Jupyter notebooks that are attached to the relevant sections throughout the thesis. All the results that have been generated have used either the Bella\footnote{\url{https://github.com/apmoore1/Bella}} or target-extraction\footnote{\url{https://github.com/apmoore1/target-extraction}} packages created during the course of the PhD. The Bella package is more focused towards the non-NN methods whereas target-extraction is only focused on the NN methods. Both packages have extensive unit tests and Bella has an easy to use out of the box functionality through its model zoo.
\end{itemize}

\section{Organisation of the Thesis}
\begin{itemize}
    \item \textbf{Chapter \ref{chapter:lit_review}: Literature Review.}\newline
    The majority of the literature review summarises the different levels of sentiment analysis starting with the most coarse grained (document level) and finishing with fine grained sentiment analysis. The review of coarse to fine grained sentiment analysis is one of, if not, the most extensive review of sentiment analysis, within the English language, showcasing the different granularities and how they link together. Within the fine grained sentiment analysis review a new extended definition for fine grained sentiment analysis is stated. Further, the literature on TDSA has been reviewed from which it states the need for and lack of a reproducibility study. Further it motivates that error splits that do exist have not been rigorously analysed to better understand what they show and when they are useful. The review finishes with some further related topics to fine grained sentiment analysis, which motivate some of the future work within the conclusion chapter.
    \item \textbf{Chapter \ref{chapter:reproducibility}: Reproducibility and Generalisability of TDSA Methods.}\newline
    It details within the introduction the motivations behind performing both the reproducibility studies and generalisation experiments. The chapter provides a more extensive related work section detailing prior work within reproducibility more broadly and then concentrating on the most related work within sentiment analysis on both reproducibility and generalisability. The methods used in both the reproducibility and generalisability are described in detail. The reproduction studies are conducted, and the results are used to answer \rqref{rq:lessons}. Lastly the generalisation experiments are performed whereby the results allow \rqref{rq:generalisable} to be answered.
    \item \textbf{Chapter \ref{chapter:methodology}: Improving Experimental Methodology for TDSA.}\newline
    This chapter contains an extensive review comparing and contrasting previous error splits within TDSA. Two new error splits are created to measure different phenomena, compared to the existing splits. All error splits are then reviewed on three English datasets, showcasing the difference between the datasets using the error splits. These error splits are then summarised describing what they do and what they hypothetically measure. The error splits are then tested across several TDSA methods and three English datasets to conclude if the error splits are measuring what was hypothesised. From these experiments, a new TDSA metric and its variants are created. The detailed review, analysis, and tests of the error splits in conjunction with the new metric allows for \rqref{rq:measured} to be answered.
    \item \textbf{Chapter \ref{chapter:case_study_methodology}: Case Studies in Improving Experimental Methodology for TDSA.}\newline
    The new experimental methodology for TDSA created within chapter \ref{chapter:methodology} is then explored through several case studies. Each case study explores a new development whereby these new development including position encoding, inter-aspect encoding, and transfer learning from a language model\footnote{Also known as contextualised word representations.}. Each development is then applied to the methods used within chapter \ref{chapter:methodology}, when appropriate, and evaluated using the new experimental methodology. Within each case study the findings from the new experimental methodology are reviewed and where appropriate will be compared to the hypothesis that motivated the relevant development.   
    \item \textbf{Chapter \ref{chapter:conclusion}: Conclusion.}\newline
    The conclusion summaries the thesis, revisits the research questions, and finishes with future work.
\end{itemize}