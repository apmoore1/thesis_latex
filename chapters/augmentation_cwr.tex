\section[Introduction]{Introduction\footnote{All code that creates the evidence for this chapter can be found here: \url{https://github.com/apmoore1/tdsa_comparisons}. Certain sections throughout this chapter may have more specific pointers to python notebooks that created the analyses within that given section.}}
\label{section:aug_introduction}
As clearly shown within table \ref{table:repro_methods_datasets_used} of the reproducibility and generalisability chapter \ref{chapter:reproducibility}, many methods have been created, although they have only been applied to selected datasets. This lack of reporting is further compounded with many prior works only stating accuracy and/or macro F1 scores on the entire dataset. This type of reporting is good at comparing methods generally on these datasets. However having more detailed analysis on different splits of the data or task specific metrics would advance the community in knowing what the methods are not representing, for instance a method could generally perform well but might perform badly when there are lots of targets in one text. Thus the field would benefit generally from detailed error analysis. Some prior works have suggested different TDSA specific splits of the entire dataset to overcome this error analysis deficiency \citep{nguyen-shirai-2015-phrasernn,wang-etal-2017-tdparse,he-etal-2018-effective,yang2018multi}, but few publications since these have used the designed splits and only report the entire dataset score. This causes a major problem within our community, and so `we do not know what we know'. To further expand on the meaning of this, a lot of the methods within the community have general scores that are fairly similar e.g. only showing 3-4\% differences on the Restaurant dataset (table \ref{tab:aug_previous_work_results})\footnote{For an up to date list of papers and scores see \textit{Papers With Code} \url{https://paperswithcode.com/sota/aspect-based-sentiment-analysis-on-semeval}.} but what does that mean? It could mean that a method is better on texts that contain multiple targets with different sentiments, or the method is good at generalising to new unseen targets. 

%Furthermore within the TDSA literature there have been many general developments with regards to improvements that are to some extent agnostic to the Neural Network (NN) architecture they are applied to. The general developments of note for this chapter are; encoding the target's position (position encoding) \citep{gu-etal-2018-position}, inter-target encoding where each target is aware of all of targets within the same text \citep{hazarika-etal-2018-modeling}, and transfer learning from Bi-directional Language Models (BiLM) \citep{sun-etal-2019-utilizing,xu-etal-2019-bert} (also known as Contextualised Word Representations (CWR) which is what they will be called from now on). Of these developments, only the first two are TDSA specific where as the transfer learning is a general machine learning concept that has been shown useful in many NLP tasks \citep{peters-etal-2018-deep}. These developments have been mainly justified by the improvements on the overall accuracy and/or macro F1 score, but the justification in the paper is normally more detailed. An example justification (and one that is typical of most developments) of inter-target encoding from \citet{hazarika-etal-2018-modeling}, where they first show improvements on the general accuracy scores over baselines. They then further state these improvements are due to the model being able to infer one target's sentiment from knowing another target's sentiment, and show this through a case study (section 4.2) from a few samples. The reason for inter-target encoding does sound valid and the case studies do help this, however qualitative case studies are hard to quantify from these papers and to a large extent impossible to compare. Furthermore in both position and inter-target encoding there have been papers by \citet{he-etal-2018-exploiting} and \citet{majumder-etal-2018-iarm} that respectively use more detailed quantitative metrics through their own error splits to justify these improvement. However the error splits used in both cases are shown within this chapter to be unsuitable as they both measure other factors that the original authors did not know they were measuring.

The focus of this chapter is to answer~\rqref{rq:measured}  `What is an appropriate empirical evaluation methodology for TDSA?'. To answer this question, we first compare and contrast the different existing error analysis splits for TDSA, as well as better formalising these splits so that they can be applied to any dataset. From these existing error splits, two novel splits and a novel metric are created. These error splits are analysed through the results of three different TDSA models and one text classification model to first justify to some extent what the splits are measuring, and secondly compare the different models performance on these splits and metrics as baselines. From the baseline results the error splits and metrics are reviewed and recommendations of what error splits and metrics should be used and why are given. 
%Finally these three baseline TDSA models are then enhanced (when appropriate) with position encoding, inter-target encoding, and CWR to quantify how these different developments enhance the models through the error splits. The results from these splits and metrics for the position and inter-target encoding enhancements will for the first time quantify the theory behind the developments, and in contrast to previous work, we use suitable error analysis splits and a more rigorous experimental setup\footnote{This is through statistical testing, comparing across more models and datasets, and in the inter-target encoding setup comparing to a more suitable baseline model.}. The results from the CWR will be the first detailed quantifiable results that should highlight what the models are not capturing and should help guide future state-of-the-art TDSA models. 
These formalised error splits and novel metrics will allow future researchers to better understand what the models are and are not capturing without having to resort to qualitative case studies and will benefit from fair, easy, and reproducible comparisons between works. Lastly to ensure all the results and analyses are performed on varying and standard datasets in the field, all experiments in this chapter are performed on three public English datasets: 1. SemEval 2014 Laptop~\citep{pontiki-etal-2014-semeval} (Laptop), 2. SemEval 2014 Restaurant~\citep{pontiki-etal-2014-semeval} (Restaurant), and 3. Election Twitter~\citep{wang-etal-2017-tdparse} (Election). All three of these datasets have been introduced in chapter \ref{chapter:reproducibility} section \ref{section:repro_experimental_setup}, and overall dataset descriptions and statistics can be found in tables \ref{table:repro_dataset_stats} and \ref{table:repro_dataset_sent_dist}.

\begin{table}[!h]
    \centering
    \input{tables/augmentation/Introduction/previous_model_results}
    \caption{Previous results for models that only use GloVe word embeddings. The \textbf{bold} and \underline{underlined} values represent the best and worse performing score for each metric and dataset. This table has been taken from \citet{du-etal-2019-capsule}.}
    \label{tab:aug_previous_work_results}
\end{table}

\FloatBarrier
\section{Error Analysis Background}
\label{section:aug_error_analysis}
As stated in the introduction of this chapter, TDSA error analysis has been lacking in both the reporting and the error splits available to analyse the methods. Therefore in this section the thesis will review the current error analysis splits available, as well as creating new error splits, and lastly stating some hypotheses around what these splits actually mean and therefore why they are useful (see table \ref{table:aug_error_split_summary} for a full summary). The hypothesis and use around the splits will then be tested in the baseline experiments subsection \ref{section:aug_baseline}.

\FloatBarrier
\subsection{Previous Work}
\label{section:aug_error_analysis_previous_work}

Currently there have only been four unique error splits suggested for TDSA and these splits can be grouped into two substantially different error split groups. The first suggested split was by \citet{nguyen-shirai-2015-phrasernn} which is based on the number of targets per text, this split created three different subsets of data: \textit{ST1} contains samples that only have one target per text, \textit{ST2} and \textit{ST3} include samples that have more than one target per text but \textit{ST2} is restricted to samples that contain one sentiment within the whole text. The second split named Distinct Sentiment (\textit{DS})\footnote{Distinct Sentiment has already been introduced and used in chapter \ref{chapter:reproducibility}, section \ref{section:repro_experimental_setup}.} by \citet{wang-etal-2017-tdparse} which is very similar to the first, is based around the number of unique sentiments per text: $DS_1$, $DS_2$, and $DS_3$ would be all the samples that contain only one, two, and three sentiments in the text, respectively. The third split which is denoted as \textit{NT} in this thesis has been used in a couple of prior works \citep{he-etal-2018-effective, zhang-etal-2019-aspect}, this split divides the data by the number of targets per text. Each of the two prior works use different subsets, \citet{zhang-etal-2019-aspect} suggests to only subset on sentences containing up to seven targets\footnote{That is seven subsets where each subset contains; 1, 2, 3, 4, 5, 6, or 7 targets per sentence.}, and \citet{he-etal-2018-effective} subsetted by sentences containing 1, 2, 3, and more than 3 targets. From the two different \textit{NT} split prior works the work by \citet{zhang-etal-2019-aspect} will be compared to the most in this thesis as it is the work that contains the most subsets (7), and thus more fine grained results. These three splits are similar as they are all based around the number of targets and for the former two splits their sentiment class within a text. Furthermore, the combination of the $DS_2$ and $DS_3$ subsets is equal to the \textit{ST3} subset. The $DS_1$ subset is equal to the combination of \textit{ST1} and \textit{ST2}. Also when there is only one target in the text $NT_1$ this is equal to \textit{ST1}. Lastly if you create subsets by conditioning on $NT_i$ and $DS_1$ where $i>1$ this would be equal to \textit{ST2}. Following on from these works, \citet{xue-li-2018-aspect} created the \textit{Hard} subset which is in fact the same as the \textit{ST3} subset\footnote{And as stated earlier in this paragraph the same as the combination of the $DS_2$ and $DS_3$ subsets} and therefore in this thesis is not counted as a new split as it is a direct derivative of past splits. All of these splits are relatively local splits in the sense that they do not take into account the global information of what is in the entire training or test datasets. Furthermore, all of these splits have been created to analyse the difficulty of a sample based on the number of targets and sentiments in the text, where it has been shown at least that more unique sentiments causes samples to be more difficult \citep{wang-etal-2017-tdparse,nguyen-shirai-2015-phrasernn} but the same cannot be said about more targets \citep{zhang-etal-2019-aspect,nguyen-shirai-2015-phrasernn}. From one of the original papers that introduced \textit{NT} \citep{zhang-etal-2019-aspect}, where the main objective of this split is to increase the number of targets explicitly and the number of unique sentiments is not taken into account, the difficulty of the samples does not increase when the number of targets increase (see figure 4 in \citet{zhang-etal-2019-aspect}).  Lastly even through the $DS_i$ split was stated to get more difficult as $i$ increased and has been shown in original work to be true \citep{wang-etal-2017-tdparse}, it has also been shown in the same work not to be true when either the method and or metric changes (see table 4 in \citet{wang-etal-2017-tdparse}). Both of these unexpected empirical findings for the \textit{NT} and \textit{DS} split will be explored empirically in the baseline results subsection \ref{section:aug_baseline}.

The \textit{ST}, \textit{DS}, and \textit{NT} splits are the first group of the two substantially different group splits, the second group of splits only contains one split by \citet{yang2018multi}, which in this thesis is called the \textit{n-shot} split. \citet{yang2018multi} were the first to explore subsets of data based on the number of times the target/aspect has appeared in the training data compared to the test. However it is not the first time in NLP that this type of error analysis has been done as this is just an adaptation of the \textit{n-shot} learning setup which has been performed in text classification \citep{zhang-etal-2019-aspect}, multi-lingual target extraction \citep{jebbara-cimiano-2019-zero}, entities and relationships within Natural Language Inference (NLI) \citep{levy-etal-2017-zero}, and finally many other NLP tasks while evaluating a language model \citep{radford2019language}. This \textit{n-shot} setup takes the test and training datasets and finds the number of times (\textit{n}) the target in the test dataset appears in the training. Therefore the subset of \textit{zero shot} ($n=0$) targets would be all the targets in the test that never appear in the training dataset. The \textit{n-shot} setup allows the method to be tested for its ability to generalise to unseen targets, and its capability of learning a new target. Thus a model that can generalise well should be able to perform well with few (or no) target examples in the training data. Furthermore, the expectation would be as \textit{n} increased the samples within that \textit{n} would get easier to classify, as it has been shown in other areas of NLP that \textit{zero-shot} ($n=0$) contains the most difficult samples \citep{jebbara-cimiano-2019-zero}. However, findings of the original work by \citet{yang2018multi} found that, in general, model performance does not correlate with \textit{n}, thus indicating that \textit{zero-shot} is no more difficult than any other \textit{n} to classify. This finding is unexpected and will be tested further in subsection \ref{section:aug_baseline}.

% Created two different groups of error splits local and global.
As suggested earlier, the main difference between these two split groups is that the first uses local information whereas the latter uses the global information between the test and training datasets. In the next subsection, the new data splits will be created to complement these existing splits. Given that the first split group contains three slightly different splits, in this thesis only the \textit{DS} and \textit{NT} splits will be used. These two were chosen as they complement each other well as the \textit{NT} split measures the effectiveness of a method with respect to the number of targets in a text and therefore their interactions. The \textit{DS} split measures a method's ability to identify target sentiment relations. Thus if a method performs well on the \textit{NT} subsets but not the \textit{DS} it suggests that it can understand when targets should interact with each other such as in the conjunction case e.g. `The \textit{battery} is really good and so is the \textit{screen}'\footnote{`battery' and `screen' are the two targets.}, but it is not good at identifying target sentiment relationships. Thus the \textit{ST} split is not required as it measures to some extent both \textit{DS} and \textit{NT}.

\FloatBarrier
\subsection{New splits}
\label{section:error_analysis_new_splits}

In this thesis, two new splits are suggested, one based on global information the other local information. The global split denoted as Target Sentiment Relation (\textit{TSR}) focuses on different ways to probe a method's ability to generalise to new targets and to new sentiment relations for already known targets. The local split denoted as Target Sentence Sentiment Ratio (\textit{TSSR}) measures the combination of \textit{DS} and \textit{NT} splits but taking into account the number of different sentiments rather the unique sentiments within the \textit{DS} split. Therefore it can be used to measure the affect to some extent of overfitting to the most frequent sentiment within a sentence. 

The \textit{TSR} split contains three subsets: 1. Known Sentiment Known Target (\textit{KSKT}), 2. Unknown Sentiment Known Target (\textit{USKT}), and 3. Unknown Targets (\textit{UT}). The first subset should be the easiest as the method will have at least some information on both the target and how it relates to that sentiment label, thus this sets the threshold for the other two subsets to meet. The second is potentially the most difficult for the method as it already knows the target and a relation to another sentiment label, but not this specific label. Therefore the \textit{USKT} split tests how well a method can generalise to new sentiment relations without overfitting to known relations for that target. The last subset is the same as the \textit{zero shot} case in the \textit{n-shot} split, thus it tests the ability of the method to generalise to new targets. This split can be seen to relate to the relation extraction task of performing zero shot entity extraction for the \textit{UT} subset and zero shot relation extraction for the \textit{USKT} subset \citep{levy-etal-2017-zero}. The results from the relation extraction literature \citep{levy-etal-2017-zero,abdou-etal-2019-x} motivates the reason why \textit{USKT} should be the most difficult split and the \textit{UT} to be easier.

The \textit{TSSR} is based on each target's sentiment frequency within a sentence thus taking into account both unique sentiments and number of targets within a sentence. Before stating how \textit{TSSR} is calculated some notation is required; given a target that is represented as $t_{ji}$ where $j$ denotes the sentence index from all of the sentences $X=\{x_1,...,x_k\}$, and $i$ denotes the index of the target within sentence $j$ where the target is $1$ of $n$ targets $T_j=\{t_{j1},...,t_{jn}\}$ within sentence $j$. Furthermore the sentiment value for $t_{ji}$ comes from the set $S=\{s_1,...,s_i\}$ of all sentiment values, where the sentiment value for a target comes from the following function $Sent(t_{ji})\subset S$. Given this notation \textit{TSSR} can be calculated from equation \ref{eq:aug_tssr}, thus the subsets within \textit{TSSR} are a continuous value ranging from $0$ to $1$. Where the minimum \textit{TSSR} value would come from one of two types of sentences:
\begin{enumerate}
    \item A sentence that contains lots of targets but few or one come from only one unique sentiment class.
    \item A sentence that contains few targets but all targets come from a different sentiment class.
\end{enumerate}

\begin{equation}
TSSR(t_{ji}) = \frac{\sum_{n=1}^{|T_j|} [Sent(t_{ji})=Sent(t_{jn})]}{|T_j|}
\label{eq:aug_tssr}
\end{equation}

Targets that are within the $1$ subset have to be either the only target within a sentence ($NT_1$) or a sentence that contains multiple target all with the same sentiment, thus the $1$ subset is equal to the combination of \textit{ST1} and \textit{ST2} subsets or the $DS_1$ subset. Any \textit{TSSR} subset that is less than $1$ must come from targets that are within texts that contain more than one unique sentiment and thus more than one target hence part of either $DS_2$ or $DS_3$ subsets. Furthermore, the assumption is that the lower the subset value the more difficult the sample will be to classify, as the target must have come from a sentence that contains lots of targets and/or the target itself has a very rare sentiment value within that sentence. For instance, a sentence that contains 3 targets and 3 different sentiments will all be part of \textit{TSSR} subset $\frac{1}{3}$. Another example, a sentence that contain 3 targets where the first is positive and the rest negative, the first will be part of \textit{TSSR} subset $\frac{1}{3}$ where as the other two parts subset $\frac{2}{3}$. TSSR's main purpose is to detect overfitting to the most frequent sentiment within the sentence, of which this can be measured by taking the absolute difference in some metric, e.g. accuracy, between high and low TSSR values. Whereby, a large absolute difference would suggest overfitting whereas a low would not, as performing well on the high TSSR values could come from just predicting the most frequent sentiment in the sentence.

The \textit{TSSR} split can be seen to be very similar to the \textit{DS} split as they both measure to some degree the number of sentiments within a sentence. However the main point of \textit{TSSR} unlike \textit{DS} is to measure overfitting to the most frequent sentiment within the sentence, where as \textit{DS} better evaluates the target sentiment relationship as the subset explicitly measure this. The \textit{TSSR} and \textit{DS} splits can be used together to measure the target sentiment relation better, as when the \textit{TSSR} shows overfitting this informs how reliable the metrics within the \textit{DS} subsets are. For example, when overfitting through \textit{TSSR} is high, the values of the \textit{DS} split may be unreliable as the model is not learning the target sentiment relationship, but just how well it is at overfitting.

Summaries of the differences in the error splits introduced in this section and section \ref{section:aug_error_analysis_previous_work} can be seen in table \ref{table:aug_error_split_summary}. Examples for each of these splits and subsets can be found in table \ref{table:aug_error_split_examples}.

\afterpage{
    \begin{landscape}% Landscape page
            \centering
            \input{tables/augmentation/error_analysis/split_table.tex}
    \end{landscape}
    \begin{table}[!ht]
        \centering
        \input{tables/augmentation/error_analysis/examples.tex}
        \caption{Examples of TDSA samples split into training and test datasets, where each example states the error split that the target will be put within. All examples have come from the Election Twitter dataset \citep{wang-etal-2017-tdparse}.}
        \label{table:aug_error_split_examples}
    \end{table}
}

\newpage
\FloatBarrier
\subsection[Analysing the splits]{Analysing the splits\footnote{All empirical tables and graphs within this section have been generated through the following notebook \url{https://github.com/apmoore1/tdsa_comparisons/blob/master/analysis/TDSA_Error_Analysis.ipynb}.}}
\label{section:aug_analysing_the_splits}
Given these five different splits \textit{DS}, \textit{NT}, \textit{TSSR}, \textit{n-shot}, and \textit{TSR}, the analyses of how often they occur within the three main English datasets that are being examined in this chapter will be explored, through this exploration it will uncover in more detail how these datasets differ. All analysis shown in this subsection is performed on the test set of the datasets, this was chosen over a validation set as no standard/formal validation sets have been created for these datasets\footnote{It has also been shown that using a standard training and test split is not optimal if the aim is to find generalisable results \citep{gorman-bedrick-2019-need,moss-etal-2019-fiesta}. Thus, showing analysis for a validation split is somewhat pointless as the best way of evaluation in the future would be to run models across multiple splits and random seeds \citep{moss-etal-2019-fiesta} when computational costs lower. For now, our evaluations only take into account random seeds subsection (\ref{section:aug_experimental_setup}).}.

The \textit{DS} split subsets can be seen in both table \ref{table:aug_error_analysis_ds} and figure \ref{fig:aug_error_analysis_ds} which clearly shows a massive difference between the Election dataset and the Restaurant and Laptop datasets. The Election dataset contains almost a 50-50 split between the $DS_1$ and $DS_2$ subsets compared to the almost 80-20 split within the Restaurant and Laptop datasets. This first shows that the Election dataset is likely to be a much more difficult dataset due to the model having to properly reflect the sentiment relation between targets and their corresponding sentiment within the text. This is in contrast to the samples that come from the $DS_1$ subset which could potentially be classified by a sentence level sentiment classifier as all the targets within that sentence all contain the same sentiment. The number of $DS_1$ samples within the Restaurant and Laptop dataset could explain why sentence level classifiers have performed well in TDSA tasks, which has been shown by numerous studies \citep{tang-etal-2016-aspect,wang-etal-2016-attention,he-etal-2018-effective,jiang-etal-2019-challenge}. Lastly, this also shows that the $DS_3$ subset, the most difficult, should not be analysed on the Laptop and Restaurant dataset due to the low number of samples. 
\begin{figure}[h!]
    \begin{floatrow}
        \ffigbox[5cm]{
          \includegraphics[scale=0.6]{images/augmentation/error_analysis/DS.png}
        }{
          \caption{Percentage of samples per \textit{DS} data subset.}
          \label{fig:aug_error_analysis_ds}
        }
        \capbtabbox{
          \input{tables/augmentation/error_analysis/DS.tex}
        }{
          \caption{Number of samples within each \textit{DS} data subset.}
          \label{table:aug_error_analysis_ds}
        }
    \end{floatrow}
\end{figure}

The \textit{NT} split, unlike the \textit{DS} split, has a continuous number of subsets as $i$ of $NT_i$ is determined by the dataset. Figure \ref{fig:aug_error_analysis_nt_continuous} shows unsurprisingly that the Election dataset compared to the other two contains far fewer samples that only have one or two targets per text, which most likely relates to the fact that it has far fewer $DS_1$ samples. Thus, showing again that the Election dataset is likely to be a more difficult dataset, as a greater number of the samples will contain sentiment that are linked through target interaction. 
\begin{figure}[h!]
    \begin{floatrow}
        \ffigbox{
          \includegraphics[scale=0.6]{images/augmentation/error_analysis/NT_continuous.png}
        }{
          \caption{Percentage of samples per $NT_i$ subset.}
          \label{fig:aug_error_analysis_nt_continuous}
        }
        \ffigbox{
          \includegraphics[scale=0.6]{images/augmentation/error_analysis/NT_discrete.png}
        }{
          \caption{Percentage of samples per \textit{NT} data subset.}
          \label{fig:aug_error_analysis_nt_discrete}
        }
    \end{floatrow}
\end{figure}
However, as can be seen in figure \ref{fig:aug_error_analysis_nt_continuous} and as stated in the work suggesting this split \citep{zhang-etal-2019-aspect} some of the subsets contain very few samples, thus any analysis on these subsets can be fairly unstable. Furthermore, as the dataset contains different ranges of $i$ in $NT_i$, binning the $NT$ split into four subsets will allow the datasets to be more comparable. The four subsets suggested would first be comprised of one subset where $i=1$ denoted. Then the other three subsets would be made up of low, medium, and high number of \textit{i}, where \textit{i} values for each of these subsets are based on the equal amount of samples per \textit{i}. Each of these subsets will be termed  \textit{1-target}, \textit{low-targets}, \textit{med-targets}, and \textit{high-targets} respectively. These subsets can be seen in figure \ref{fig:aug_error_analysis_nt_discrete} and table \ref{table:aug_error_analysis_nt}, along with the values of \textit{i} that each subset represents in table \ref{table:aug_error_analysis_nt_range}. The \textit{1-target} subset denotes the methods baseline performance when not having to consider target interaction, which to some extent should set the upper bound for all other subsets. The other three subsets represent low to high likelihood of requiring target interaction knowledge to classify the samples correctly. Thus, a method that can model these interactions well should perform well across all four subsets, whereas a method that cannot will perform increasingly better from high \textit{i} to $i=1$.
\begin{table}[!ht]
    \centering
    \input{tables/augmentation/error_analysis/NT.tex}
    \caption{Number of samples per \textit{NT} subset.}
    \label{table:aug_error_analysis_nt}
\end{table}

\begin{table}[!ht]
    \centering
    \input{tables/augmentation/error_analysis/NT_Target_Range.tex}
    \caption{Range of \textit{i} values that represent each \textit{NT} subset.}
    \label{table:aug_error_analysis_nt_range}
\end{table}

The \textit{TSSR} split has a continuous number of subsets based on the dataset, of which the \textit{TSSR} subset values can range from $1$ to $0$. These \textit{TSSR} subsets can be seen in full in figure \ref{fig:aug_error_analysis_tssr_full} for each dataset. As noted before in section \ref{section:error_analysis_new_splits} the $1$ \textit{TSSR} subset is equal to the $DS_1$ subset of which this subset dominates the Laptop and Restaurant datasets. Thus for \textit{TSSR} value subsets less than $1$ the Laptop, Restaurant, and Election datasets contain $\sim20\%$, $\sim24\%$, and $\sim55\%$ of their samples respectively. Furthermore, it can be seen clearly from figure \ref{fig:aug_error_analysis_tssr_small} and \ref{fig:aug_error_analysis_tssr_full} that only the Election dataset has a large number of samples that have a \textit{TSSR} value less than or equal to $0.5$. These lower \textit{TSSR} values ($\leq 0.5$) represent the least or joint equal frequent sentiment within the text when the text comes from the $DS_2$ subset of which the majority of samples that contain more than one unique sentiment do come from the $DS_2$ for all datasets. Furthermore the lower \textit{TSSR} value subsets are going to be by far the more difficult samples to predict for, due to them containing targets that meet at least one of the two following criteria:
\begin{enumerate}
    \item The target coming from a text that contains lots of other targets but the target itself is within the least dominating sentiment class within the text.
    \item The target comes from a text that contains few targets but all targets come from a different sentiment class.
\end{enumerate}
Thus these two criteria therefore will measure the method's ability to identify target sentiment relations in a text that is dominated by other target sentiments.

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.42]{images/augmentation/error_analysis/tssr_full_range.png}
    \caption{The right (left) plot shows the cumulative sample count (percentage) for decreasing values of \textit{TSSR}.}
    \label{fig:aug_error_analysis_tssr_full}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.42]{images/augmentation/error_analysis/tssr_small_range.png}
    \caption{The left plot shows the cumulative sample count percentage for decreasing values of \textit{TSSR} starting from $0.5$. The right plot shows the same as the left but for the full range of \textit{TSSR} values but excluding the Election dataset.}
    \label{fig:aug_error_analysis_tssr_small}
\end{figure}

As can be seen from the figures of \ref{fig:aug_error_analysis_tssr_small} and \ref{fig:aug_error_analysis_tssr_full} the \textit{TSSR} value subsets are different depending on the dataset just like the \textit{NT} split. Therefore, the split will contain four subsets to represent different levels of difficulty and allow the performance on the subsets to be comparable across datasets. The four subsets are the following:

\begin{enumerate}
    \item \textit{1-TSSR} contains all targets that come from a sentence that only contain that one target thus this is the same as the $NT_1$/\textit{NT 1-target} subset and \textit{ST1} subset from section \ref{section:aug_error_analysis_previous_work}. 
    \item \textit{1-multi-TSSR} contains all targets that come from a sentence that contains more than one target but they all have the same sentiment value. This is the same as the difference of the $DS_1$ subset and the $NT_1$/\textit{NT 1-target} subset, and the same as the \textit{ST2} subset from section \ref{section:aug_error_analysis_previous_work}.
    \item \textit{high-TSSR} contains all of the targets that are within the following \textit{TSSR} value range $m \le x < 1$. 
    \item \textit{low-TSSR} contains all of the targets that are within the following \textit{TSSR} value range $0 \le x < m$.
\end{enumerate}

The value for $m$ is dataset specific and is based on ensuring that the \textit{low-TSSR} values contain at least 50\% of the samples after the \textit{1-TSSR} and \textit{1-multi-TSSR} samples have been removed from the dataset. The rest of the samples are given to the \textit{high-TSSR} subset. 

The \textit{1-TSSR} subset explores the effect of having no target interaction nor complex target sentiment relation due to only having one target and one sentiment. The \textit{1-multi-TSSR} explores the effect of having target interaction with potentially easier target sentiment relation due to the targets having all of the same sentiment, thus allowing the method to get the target sentiment relation incorrect without any consequence. The \textit{high-TSSR} and \textit{low-TSSR} measures the effect of increasing the target interaction and target sentiment relation to a smaller and larger degree respectively. 

The \textit{1-multi-TSSR} is expected to be the easiest subset as a method can make use of the target interaction and incorrectly inferring the correct target sentiment relationships as all targets will have the same sentiment. The next easiest subset would be the \textit{1-TSSR} due to no target interaction nor complex target sentiment relationships, however more difficult than \textit{1-multi-TSSR} as the method will have less sentiment signal within the text. Lastly the \textit{high-TSSR} and then \textit{low-TSSR} should be the most difficult subsets in that order due to target interactions and complex target-sentiment relationships. Furthermore, as stated earlier in section \ref{section:error_analysis_new_splits} the main purpose of this split is to measure overfitting to most frequent sentiment within the text. Through the subsets the overfitting could be detected if the method performs better in the \textit{1-multi-TSSR} than the \textit{1-TSSR}, this could show that either the method is exploiting the frequent sentiment situation or it is better at capturing the target interaction. However, the difference in performance between the \textit{high-TSSR} and \textit{low-TSSR} could detect this overfitting better due to the \textit{low-TSSR} coming from the most infrequent sentiment within the texts and the \textit{high-TSSR} coming from the most frequent. Thus if the difference between the \textit{high-TSSR} and the \textit{low-TSSR} is large then most frequent sentiment overfitting is more than likely occurring.

Within figure \ref{fig:aug_error_analysis_tssr_subsets} and table \ref{table:aug_error_analysis_tssr_subsets} is the breakdown of the number of samples per \textit{TSSR} subset, table \ref{table:aug_error_analysis_tssr_subsets_range} shows the \textit{TSSR} value range for each subset\footnote{\textit{1-multi-TSSR} is not within this table as it has the same range as \textit{1-TSSR}.} (the gap in range between each subset exists because there are no samples that contain a \textit{TSSR} value in that gap). As shown in the figure and tables, the Election dataset compared to the other two contains far fewer \textit{1-TSSR} samples compared to the number of samples in \textit{1-multi-TSSR}. Furthermore, the Election dataset also contains a large \textit{high-TSSR} subset, and low compared to the other datasets. 

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.6]{images/augmentation/error_analysis/tssr_subsets.png}
    \caption{Percentage of samples per \textit{TSSR} subset.}
    \label{fig:aug_error_analysis_tssr_subsets}
\end{figure}

\begin{table}[!ht]
    \centering
    \input{tables/augmentation/error_analysis/TSSR_subsets.tex}
    \caption{Number of samples within each \textit{TSSR} subset.}
    \label{table:aug_error_analysis_tssr_subsets}
\end{table}

\begin{table}[!ht]
    \centering
    \input{tables/augmentation/error_analysis/TSSR_subset_value_range.tex}
    \caption{Range of \textit{TSSR} values for each \textit{TSSR} subset.}
    \label{table:aug_error_analysis_tssr_subsets_range}
\end{table}

To further understand the relationship between the \textit{TSSR} split and the \textit{NT} and \textit{DS} splits figure \ref{fig:aug_error_analysis_tssr_ds_nt_breakdown} shows the breakdown of each \textit{TSSR} subset by the \textit{NT} and \textit{DS} splits. As can be seen from the figure for the \textit{1-multi-TSSR} subset, the Election dataset has a more even distribution of samples that come from sentences that contain 2, 3, and 4 targets. Compared to the Restaurant and Laptop datasets where the majority of samples contain only 2 targets and then dramatically decreases as \textit{NT} increases. For all datasets the \textit{high-TSSR} subset contains relatively more samples that come from texts that contain a larger number of targets compared to the \textit{low-TSSR} subset. This is most likely due to the fact that targets with rare sentiment within the target's sentence only occur once or twice in a large \textit{NT} sentence where as the lesser rare sentiment targets occur far more often in those sentences and are counted within the \textit{high-TSSR} subset. An example of this can be thought of where the sentence contains 5 targets of which 1 comes from the positive sentiment class and the rest negative, thus the \textit{high-TSSR} will have 4 samples where as the \textit{low-TSSR} only 1 from the sentence coming from the 5 \textit{NT} subset. Unsurprisingly the majority of the limited number of $DS_3$ samples are almost all inclusively within the \textit{low-TSSR} subset for all datasets. Figure \ref{fig:aug_error_analysis_tssr_ds_nt_breakdown} also shows that the main difference between the three subsets is less about the distribution of \textit{NT} but rather the distribution of sentiment labels within a sentence. Lastly, figure \ref{fig:aug_error_analysis_tssr_ds_nt_breakdown} explains to some degree the reason why Election and Restaurant datasets have a large \textit{TSSR} value range as shown in table \ref{table:aug_error_analysis_tssr_subsets_range}, as these datasets must contain sentences that have a large number of targets as well as those sentences containing a different number of sentiments. 

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.52]{images/augmentation/error_analysis/tssr_nt_ds.png}
    \caption{Percentage of samples per \textit{TSSR} subset broken down by \textit{NT} and \textit{DS} splits.}
    \label{fig:aug_error_analysis_tssr_ds_nt_breakdown}
\end{figure}

Similar to \textit{NT}, and \textit{TSSR} the \textit{n-shot} split has different values of \textit{n} based on the dataset, of which this can be best seen in figure \ref{fig:aug_n_shot_cuml}. From the left plot in figure \ref{fig:aug_n_shot_cuml} the sharpness of each curve for each dataset would appear to relate to the size of the dataset, as Laptop is the smallest and has the sharpest curve, whereas Election is the largest and has the least steep curve. Furthermore, in the left plot it can be seen that both Restaurant and Election start to flatten off around 80\% and 64\% suggesting that the test dataset contains a lot of samples with targets that have been very frequently seen in the training dataset. Thus these high \textit{n} samples should be easy to classify due to the method having seen lots of samples of those targets in the training dataset. To better capture how many samples with no target in training data appear compared to those with targets that appear very often, figure \ref{fig:aug_n_shot_low_med_high} contains three plots showing the low, medium, and high frequencies of \textit{n} (note that the values on the Y-axis are different). From this we can clearly see that for all datasets the largest number of samples occurs when $n=0$. Lastly, it shows that both the Restaurant and Election datasets contain a lot of samples where the targets have been seen very frequently in the training dataset.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.42]{images/augmentation/error_analysis/n_shot_cuml.png}
    \caption{The right (left) plot shows the cumulative sample count (percentage) for increasing values of \textit{n}.}
    \label{fig:aug_n_shot_cuml}
\end{figure}

This analysis suggests that the \textit{n-shot} split should be broken up into four different subsets rather than \textit{n} subsets, similar to the \textit{NT} split. This is suggested as a comparison of \textit{n} subsets with respect to some metric can be very unstable as some of these \textit{n} subsets can contain very few samples e.g. when $n=12$ for the Laptop dataset (see figure \ref{fig:aug_n_shot_low_med_high}). The four subsets suggested would first comprise of one subset where $n=0$ which is the \textit{UT} and \textit{zero-shot} case. Then the other three subsets would be made up of low, medium, and high number of \textit{n}, where \textit{n} values for each of these subsets are based on the equal amount of samples per \textit{n}. Each of these subsets will be termed \textit{zero-shot}, \textit{low-shot}, \textit{med-shot}, and \textit{high-shot} respectively. These subsets can be seen in figure \ref{fig:aug_error_analysis_n_shot_discrete} and table \ref{table:aug_error_analysis_n_shot}, along with the values of \textit{n} that each subset represents in table \ref{table:aug_error_analysis_n_shot_n_relation}. Lastly these subsets should allow for comparability across datasets and better analysis of the method's ability of learning a new target, as a good method should have a steady high performance across all of these subsets, whereas a method that overfits to targets would have a decreasing performance from the high to zero shot subsets.

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.6]{images/augmentation/error_analysis/n_shot_discrete.png}
    \caption{Percentage of samples per \textit{n-shot} data subset.}
    \label{fig:aug_error_analysis_n_shot_discrete}
\end{figure}

\begin{table}[ht!]
    \centering
    \input{tables/augmentation/error_analysis/n_shot.tex}
    \caption{Number of samples per \textit{n-shot} subset.}
    \label{table:aug_error_analysis_n_shot}
\end{table}

\begin{table}[ht!]
    \centering
    \input{tables/augmentation/error_analysis/n_shot_n_relation.tex}
    \caption{Range of \textit{n} values that represent each \textit{n-shot} subset.}
    \label{table:aug_error_analysis_n_shot_n_relation}
\end{table}

\begin{sidewaysfigure}[!ht]
    \centering
    \includegraphics[scale=0.3]{images/augmentation/error_analysis/n_shot_low_med_high.png}
    \caption{Number of samples as a percentage per value of \textit{n}.}
    \label{fig:aug_n_shot_low_med_high}
\end{sidewaysfigure}

\textit{TRS}, the last and new split to be analysed can be best seen through figure \ref{fig:aug_error_analysis_trs} and table \ref{table:aug_error_analysis_trs}. As can be seen the \textit{UT} subset is almost the largest subset for the smallest dataset (Laptop) but a relatively small subset for the largest dataset, of which this has already been seen through the \textit{zero-shot} subset. Furthermore, the \textit{USKT} is the smallest subset by far across the datasets, however it still counts for at least 10\% of all data in the Laptop dataset. Based on the two global splits, \textit{TRS} and \textit{n-shot}, they show the importance of a method to generalise to new targets (\textit{UT} and \textit{zero-shot} cases) and new relations (\textit{USKT}) which will more likely occur in a low resource setting, which can be best seen by the size of these subsets on the Laptop dataset. 

\begin{figure}[ht!]
    \begin{floatrow}
        \ffigbox{
          \includegraphics[scale=0.6]{images/augmentation/error_analysis/TRS.png}
        }{
          \caption{Percentage of samples per \textit{TRS} data subset.}
          \label{fig:aug_error_analysis_trs}
        }
        \capbtabbox{
          \input{tables/augmentation/error_analysis/TRS.tex}
        }{
          \caption{Number of samples within each \textit{TRS} data subset.}
          \label{table:aug_error_analysis_trs}
        }
    \end{floatrow}
\end{figure}

\FloatBarrier
\subsection{Conclusion so far}
Within this subsection the different error splits that exist within the TDSA literature have been reviewed and added too. Furthermore, each of these splits have been analysed on three major datasets showing for the first time how each dataset has different characteristics. The local splits \textit{DS}, \textit{NT}, and \textit{TSSR} show that the Election dataset contains far more unique sentiments per text, whereas the Restaurant dataset contains the most targets per text. The global splits, \textit{n-shot} and \textit{TRS}, find that in the low resource setting a method that can generalise well to new targets and new sentiment relations would be important, which can be seen through the contrast in subset sizes between Election and Laptop datasets. Lastly, table \ref{table:aug_error_split_summary} summarises the differences in the error splits, examples for each of these splits and subsets can be found in table \ref{table:aug_error_split_examples}. Finally, a summary of all of the statistical breakdowns of each of these splits can be found in table \ref{table:aug_error_analysis_summary_stats}.

\begin{table}[!ht]
    \centering
    \input{tables/augmentation/error_analysis/summary_stats.tex}
    \caption{Summary statistics of all splits}
    \label{table:aug_error_analysis_summary_stats}
\end{table}

\FloatBarrier
\section{Method Performance on the Error Splits}
\subsection{Introduction}
\label{section:aug_method_performance_intro}
In this sub-section the performance of the following four NN based methods will be analysed across the five different error splits stated in section \ref{section:aug_error_analysis}. The results from these experiments will be used to further analyse what the error splits show as well as create multiple baselines for these splits. The four different methods are the following: 
\begin{enumerate}
    \item \textit{CNN} -- The sentence level CNN from \citet{kim-2014-convolutional} that encodes the context/sentence and does not take into account the target.
    \item \textit{TDLSTM} \citep{tang-etal-2016-effective} -- An LSTM that encodes the target by ensuring the target word(s) are always the last word(s) fed to the LSTM from the context/sentence. Thus a position based method where the position is encoded through the NN architecture. This is the same TDLSTM as that from chapter \ref{chapter:reproducibility}, described in detailed within section \ref{section:repro_lstm_description}.
    \item \textit{IAN} \citep{ma2017interactive} -- Encodes the target into the context via attention.
    \item \textit{Att-AE} -- A model that is the same as the \textit{AE} model from \citet{wang-etal-2016-attention} but with an attention layer after the LSTM enocder. This model is also the same as the inter-aspect model (from now on called \textit{Inter-AE}) from \citet{hazarika-etal-2018-modeling} but without the LSTM aspect encoder (phase 2 in figure 1) that models other targets from the same context/sentence.  
\end{enumerate}
Thus to summarise the differences in the TDSA methods (last three methods from above); \textit{TDLSTM} encodes the position of targets through its architecture, \textit{IAN} encodes the target into the context via attention and also encodes the context into the target through attention, and \textit{Att-AE} encodes the target into the context through concatenation of the target vector onto each word vector within the context before the LSTM encoder. The \textit{Att-AE} does perform attention over the context but unlike \textit{IAN} does not explicitly model the target in the attention of the context nor does it perform attention over the target word(s)\footnote{For the detailed reader, the \textit{IAN's} attention can be denoted as \textit{general} where as \textit{Att-AE} would be \textit{concat} based on the notation from \citet[\S3.1]{luong-etal-2015-effective}.}.

%Furthermore as stated in the introduction section \ref{section:aug_introduction}, the TDSA methods will be enhanced with three different developments; 1. position-encoding, 2. inter-target encoding, and 3. CWR. From the three methods, only \textit{IAN} and \textit{Att-AE} will be enhanced with position-encoding as \textit{TDLSTM} already has the target position somewhat encoded into its NN architecture. Each of these enhancements will be tested against their standard/base model with respect to the error analysis splits. The quantitative results from these error analysis splits will be compared to the original qualitative justifications for these developments from the respective papers.

The reason why \textit{Att-AE} is neither exactly \textit{AE} \citep{wang-etal-2016-attention} nor \textit{Inter-AE} \citep{hazarika-etal-2018-modeling} is due to not wanting to add inter-target encoding to the baseline models, as within chapter \ref{chapter:case_study_methodology} all models are going to have inter-target encoding added. Furthermore as the model will have inter-target encoding added in chapter \ref{chapter:case_study_methodology} it will convert \textit{Att-AE} to \textit{Inter-AE} thus making it a standard model from prior literature. The use of different methods from those within chapter \ref{chapter:reproducibility} is due to the surge in purely NN based TDSA methods of recent years, thus the only methods used within this chapter are NN based.

Due to having a non-target method \textit{CNN}, all of the TDSA methods have a baseline to compare against. Furthermore as all experiments will contain results from at least two TDSA methods to a larger extent the results should generalise to different TDSA NN architectures. In the following sections the thesis will:
\begin{enumerate}
    \item State the experimental setup of all experiments within this section (section \ref{section:aug_experimental_setup}).
    \item Explore the differences in performance on the error splits across the methods (section \ref{section:aug_baseline}).
    %\item Evaluate the error split across three different model enhancements position encoding (section \ref{section:aug_position_encoding}), inter target encoding (section \ref{section:aug_inter_target_encoding}), and CWR (section \ref{section:aug_cwr}).
\end{enumerate}

\FloatBarrier
\subsection[Experimental Setup]{Experimental Setup\footnote{The code to generate table \ref{table:aug_methods_performance_split_breakdown} can be found in the README at the following \url{https://github.com/apmoore1/tdsa_comparisons\#analysis-of-the-datasets}. The code to generate tables \ref{table:aug_methods_performance_global_error_diff} and \ref{table:aug_methods_performance_local_error_diff} can be found in the following notebook \url{https://github.com/apmoore1/tdsa_comparisons/blob/master/analysis/TDSA_Error_Analysis.ipynb}.}}
\label{section:aug_experimental_setup}
As stated in the introductory section of this chapter (\ref{section:aug_introduction}), the three datasets that are used throughout this chapter are the Election, Laptop and Restaurant datasets. However, unlike the error analysis section (\ref{section:aug_error_analysis}), the standard training split for each of the datasets will be further randomly split into a new training and an additional validation split, of which the size of these splits can be seen in table \ref{table:aug_methods_performance_split_breakdown}. The validation set is required so that the early stopping can be used for all of the NN methods. Furthermore, the validation set would usually be used for more hyper-parameter tuning, e.g. finding the best learning rate etc, but due to compute time this is not the case. Instead we selected the most common hyper-parameters from the literature as detailed in table \ref{table:aug_methods_performance_default_hyperparameters}, it will be stated explicitly within this chapter if these hyperparameters are not used. One default hyperparameter of note is the embedding, of which the 840 billion token 300 dimensional GloVe vector \citep{pennington-etal-2014-glove} (from now on called GloVe and is called that in table \ref{table:aug_methods_performance_default_hyperparameters})\footnote{This is the same 300 dimension GloVe embedding that is used in chapter \ref{chapter:reproducibility}.} was chosen, as it is the most common default embedding in the TDSA literature. All text will be tokenised using Spacy and then lower-cased\footnote{The text was lower-cased as none of the three TDSA methods stated in their original works if they lower-cased the text's or not. The assumption here is that they did.}. Lastly, all results reported in this section will be results on the test set and all validation results will be reported in the appendix for reproducibility reasons \citep{dodge-etal-2019-show}. However if there is a large difference between the validation and test results this will be mentioned explicitly in this section.

\begin{table}[h!]
    \centering
    \input{tables/augmentation/methods_performance/dataset_breakdown.tex}
    \caption{Number of samples.}
    \label{table:aug_methods_performance_split_breakdown}
\end{table}

Due to the splitting of the training dataset, the error analysis split statistics in section \ref{section:aug_error_analysis} will not be identical for the global error splits (\textit{n-shot} and \textit{TRS}) between the train/test and train/validation as they rely on a comparison of train and validation/test. Even though they will not be identical they are relatively similar as shown by table \ref{table:aug_methods_performance_global_error_diff}. Furthermore, as the local splits (\textit{DS}, \textit{NT}, and \textit{TSSR}) are only reported for the test set, table \ref{table:aug_methods_performance_local_error_diff} shows them for the validation and test set showing that they are again relatively similar, thus results should be comparable between validation and test sets.

Furthermore, for all of the experiments performed in this chapter each model will have trained/ran on the respective data eight times. Thus allowing for the random seed problem, that is known in NN methods within NLP \citep{reimers-gurevych-2017-reporting}, and to be able to perform statistical significance tests that take into account this problem \citep{reimers2018comparing}\footnote{These significance tests are different to those used in chapter \ref{chapter:reproducibility}, as these for the neural network based methods better take into account all runs produced by different random seeds.}. \citet{reimers2018comparing} has shown that by using a minimum of eight runs two models can be compared with a confidence level of 99\% which is equivalent to $p \leq 0.01$ no matter if the scores from those runs comes from a non-normal distribution. The two scoring metrics commonly used in TDSA and will be used in this chapter are accuracy and macro F1, of which only accuracy can be assumed to produce scores originating from a normal distribution and thus can use the more powerful parametric tests \citep{dror-etal-2018-hitchhikers}. Therefore following \citet{reimers2018comparing} for the accuracy scores the significance test used will be the Welch's t-test (parametric test) \citep{welch1947generalization} and for macro F1 the Wilcoxon signed-rank test (non-parametric test) \citep{wilcoxon1992individual}. When comparing two models using these statistical tests for each test the one-tailed version of it will be used as in these experiments the requirement is only to know if one model is better than another. In this and the next chapter \ref{chapter:case_study_methodology}, a method is defined as the general NN architecture where as the model is defined as the concrete configuration of that method. Thus two models can be different but use the same method, for example the difference would be the word vectors that the two models use. When comparing two models across multiple variables and therefore significance tests a correction procedure is required, as explained in section \ref{section:repro_neural_pooling}. In this chapter, the Bonferroni correction procedure will be used where appropriate as in none of our cases can independence be assumed.

%Before going into the details of comparing two models across multiple variables, we first need to define what the differences are between a method and a model. A method is the general NN architecture where as the model is the concrete configuration of that method. Thus two models can be different but use the same method, for example the difference would be the word vectors that the two models use. When comparing two model across multiple variables and therefore significance tests a correct procedure is required, as explained in section \ref{section:repro_neural_pooling}. In this chapter, Bonferroni correction procedure will be used where appropriate as in none of our cases can independence be assumed.   %When comparing two models the value of $\alpha$ in $p \leq \alpha$ is used to define the number of type 1 errors in the statistical test. However when comparing two models across multiple variables and therefore multiple significance tests if the number of times a test is defined as significant are simply counted the number of type 1 errors in the tests will increase \citep{dror-etal-2017-replicability}. Thus simply counting the number of times a test is significant cannot be used without further introducing type 1 errors. To bound these type 1 errors across multiple tests requires a correction mechanism of which \citet{dror-etal-2017-replicability} recommends Bonferroni (Fisher) when the variables are dependent (independent), where Fisher is the more powerful version. In this chapter, Bonferroni will be used where appropriate as in none of our cases can independence be assumed.

\FloatBarrier
\subsection[Baseline Results]{Baseline Results\footnote{All tables and graphs within this section have been generated through the following notebook \url{https://github.com/apmoore1/tdsa_comparisons/blob/master/analysis/TDSA_Baseline_Results.ipynb}. The exception to this are the tables generated within appendix \ref{section:appendix_cnn_tdsa_baseline}, of which there is a pointer in that appendix to the relevant notebook.}}
\label{section:aug_baseline}
\subsubsection{Introduction}
The baseline results use the four different methods stated in section \ref{section:aug_method_performance_intro} as is without any changes to their respective NN architectures. These results will be explored to see whether the intuition behind the error splits as stated in section \ref{section:aug_analysing_the_splits} is to a degree true. 

For clarification, the sentence/text level \textit{CNN} method is trained differently to the TDSA methods due to the fact that it does not model the target within the sentence. Thus instead of training the \textit{CNN} method with potentially the same sentence multiple times with potentially multiple different sentiments as is the case with TDSA datasets\footnote{This was how the non-target aware models from chapter \ref{chapter:reproducibility} were trained. Therefore the non-target aware models in some training samples would have been given the same sentence with different sentiments to train on. Even though it may have been better to train them in the ways stated within this section, this was not tested due to compute time. Furthermore it was assumed that they were trained in the same way as their target aware methods as it was not stated in the papers \citep{vo2015target, tang-etal-2016-effective, wang-etal-2017-tdparse} that were reproduced that they were trained any differently.}, the TDSA dataset is converted to a text level dataset. To convert from TDSA to a text dataset each text/sentence can only contain one sentiment, from this two options are plausible; 1. only uses texts that contain one unique sentiment ($DS_1$ dataset), or 2. use the majority sentiment from the text. These two options were compared of which the detailed results can be found in appendix \ref{section:appendix_cnn_tdsa_baseline}, of which it was found that the second/later option performed best on 2 of the 3 datasets and all datasets for the accuracy and macro f1 metrics respectively across both validation and test splits. For clarification, when predicting with this text level classification method all targets within the same text will be given the predicted text level sentiment label. This experiment of comparing the two options of training a text classifier on TDSA data was required as prior works that have shown results for text classifiers never explain how they were trained \citep{tang-etal-2016-aspect,wang-etal-2016-attention,he-etal-2018-exploiting,jiang-etal-2019-challenge}. For a stronger text classifier baseline one could consider pre-training the text classifier from large sources of annotated data such as Yelp reviews \citep{tang-etal-2015-learning}, Amazon reviews \citep{mcauley2015image,he2016ups}, or Tweets using distant supervision \citep{go2009twitter} for the Restaurant, Laptop, and Election datasets respectively and then fine tune them on the respective TDSA dataset. However this stronger baseline is not considered in this work as we are not looking at transfer learning from other sources of sentiment, but this has been shown to be beneficial for TDSA methods \citep{he-etal-2018-exploiting}.  

\FloatBarrier
\subsubsection{Overall Results}
Figure \ref{fig:aug_baseline_overall_scores} shows the results of the baseline models across both the validation and test splits with the associated tables \ref{table:aug_methods_baseline_tdsa_metrics_validation} and \ref{table:aug_methods_baseline_tdsa_metrics_test} in appendix \ref{appendix_augmentation_tables}. From these results it can be seen that the \textit{CNN} text classification baseline is indeed a strong baseline for the Laptop and Restaurant datasets that the TDSA methods find difficult to beat. 

\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.4]{images/augmentation/methods_performance/baseline/baseline_overall_scores.pdf}
    \caption{The mean and standard deviation error bars from running each model 8 times.}
    \label{fig:aug_baseline_overall_scores}
\end{figure}

\FloatBarrier
\subsubsection{Comparison of the Original Model Scores to the Reproduced Models}
Figure \ref{fig:aug_baseline_replication_scores} compares the single run performance of the original TDSA models scores from their associated papers to the distribution of eight accuracy scores from our reproduced TDSA methods\footnote{Accuracy metric was the only metric reported in all of the original TDSA method papers and none of them reported on the Election dataset.}. As can be seen from figure \ref{fig:aug_baseline_replication_scores} the models original score are within the distribution of scores from the reproduced models apart from \textit{IAN} where the original models performance is a lot higher especially for the Laptop dataset. \textit{IAN's} performance difference is most likely due to the fact that the reproduced version uses a different optimiser, ADAM \citep{kingma2014adam}, instead of SGD with momentum \citep{qian1999momentum}, this design choice was made so that all models used the same optimiser. Even though it would be good to optimise the performance of the \textit{IAN} model so that it produces scores similar to the original doing so in a fair manner would mean hyperparameter tuning the other models as well \citep{dodge-etal-2019-show}, which would start becoming computationally expensive. Thus in this thesis it is accepted that the \textit{IAN} model has not been reproduced to the same performance as the original paper reports.

\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.4]{images/augmentation/methods_performance/baseline/replication_experiment_baseline.pdf}
    \caption{Distribution of all scores and the line represents the mean value, of which for the original models this line represents their only reported score. Model names with a \textit{(O)} represent the score reported in the original models paper.}
    \label{fig:aug_baseline_replication_scores}
\end{figure}

\FloatBarrier
\subsubsection{Overall Results Comparison between TDSA and Text Classification Models}
This thesis shows for the first time that not all TDSA models are statistically significantly better than a text classifier as shown by table \ref{tab:augmentation_baseline_tdsa_text_classifier_p_values}, even though all three original TDSA papers state that their TDSA models are superior to a text classifier. Furthermore, at the $95\%$ confidence level none of the TDSA methods are significant on the Laptop test split no matter what the metric is. This shows that potentially hyperparameter tuning is very important to get the most out of the TDSA models. More likely the reason for the text classifier's strong performance on the Laptop and Restaurant dataset compared to the Election dataset is that these datasets contain a large quantity of $DS_1$ samples (see figure \ref{fig:aug_error_analysis_ds}), of which it is shown later in this section in figure \ref{fig:aug_baseline_test_error_subset} that the text classifier does at least as good if not better than the TDSA models on the $DS_1$ subset in the Laptop and Restaurant datasets. This further shows that the overall metrics tell us very little in what the difference is between a text classifier model and the TDSA models.  
%\citet{jiang-etal-2019-challenge} concluded as well for the Restaurant dataset\footnote{\citet{jiang-etal-2019-challenge} did not perform any significance tests between existing TDSA methods and the text classifier baselines.} that the text classifier good performance was due to the large quantity of text. Furthermore as shown 



\begin{table}[ht!]
    \centering
    \input{tables/augmentation/methods_performance/Baselines_results/Baseline_text_classifier_p_values.tex}
    \caption{The P-values for each model where the null hypothesis is that each model performs as well as a \textit{CNN} text classifier. The P-Values in bold are those $\leq 0.05$.}
    \label{tab:augmentation_baseline_tdsa_text_classifier_p_values}
\end{table}

\FloatBarrier
\subsubsection{Error Split Results}
\label{section:augmentation_cwr_error_split_results_sub}

The performance of all of the models across all datasets for each split and their associated subsets can be seen in figure \ref{fig:aug_baseline_test_error_subset} (appendix \ref{appendix_augmentation_figures} figure \ref{fig:aug_baseline_validation_error_subset} shows the validation split results). The figures that contain subset performance results will not contain results for the $DS_3$ subset for the Laptop and Restaurant datasets, this is due to the subset containing very few samples as highlighted in subsection \ref{section:aug_analysing_the_splits}. The error split results for the test and validation splits are better highlighted in figures \ref{fig:aug_baseline_test_error_diff_subset} and \ref{fig:aug_baseline_validation_error_diff_subset} respectively where the accuracy on the whole dataset (overall accuracy) is subtracted from the error subset accuracies. These figures can thus evaluate the error splits that were discussed and created within this section. In the list below the results will be analysed by error split:

\begin{itemize}
    \item \textit{DS} split, as expected, increases in difficulty as the number of unique sentiments in the text increases, thus showing that target sentiment relation to be a difficult task for the models to perform. Furthermore, it can be seen that on average the \textit{TDLSTM} model performs consistently well on the $DS_2$ and $DS_3$ subsets compared to the other models.
    \item \textit{NT} split does not have a consistent affect on the performance of the models, this was also found in one of the original papers \citep{zhang-etal-2019-aspect}. One would expect texts that contain a lot of targets to be more difficult and at times it is as shown by the Restaurant dataset. However, on the other two datasets this is not the case. Furthermore, the performance across the subsets can differ between datasets splits, e.g. the performance of all models on the \textit{med-targets} subset on the Election dataset is worse than \textit{high-targets} for all models on the test split, but on the validation figure (\ref{fig:aug_baseline_validation_error_diff_subset}) the opposite is true. This would suggest that even though theoretically a text with more targets should be more difficult for a model to classify, due to the complexities of matching targets to their respective sentiments \citep{zhang-etal-2019-aspect}, this is not the case. Thus later in this section, analysis is conducted to investigate what other factors are influencing the change in performance within the \textit{NT} split. However, for now it can be concluded that \textit{NT} cannot effectively evaluate the target interaction as no consistent trend can be found in this split.
    \item \textit{TSSR} As expected the \textit{1-Multi} subset is by far the easiest subset to classify suggesting that the models are exploiting the fact that all targets have the same sentiment. The \textit{1} subset tends to perform the next best with the \textit{high} subset at times quite close if not the same. A reason for the \textit{high} subset to have such high performance across the models could be due to the models overfitting to the most frequent sentiment class in the text, as suggested in section \ref{section:aug_error_analysis}. As expected the \textit{low} subset is by far the worst across all datasets and models and in some cases harder to classify than samples within $DS_2$ and $DS_3$. Only on the Laptop test split are the \textit{high} scores similar to the \textit{low}, of which this might be due to the lack of samples for the \textit{high} subset (5\% of the dataset) compared to the (11.1\% of the dataset) in the \textit{low} subset, which is also suggested by the large error bars. Furthermore the sentiment overfitting which this split is supposed to measure does show to some extent where the TDSA model, \textit{TDLSTM}, that performs consistently better or at least as good in the $DS_2$ and $DS_3$ subsets tends to have a smaller difference between subset \textit{1-Multi} and \textit{1}, and is consistently a lot higher than the text classifier on the \textit{low} subset. However this split does not measure sentiment overfitting explicitly very well without the text classification baseline and the \textit{DS} split. For example without \textit{DS} and the text classification baseline it would be impossible to know that the \textit{TDLSTM} is performing target sentiment relation well on the laptop dataset as the $DS_2$ subset performance could be high due to \textit{TDLSTM} predicting the most frequent sentiment class. This cannot be the case as the performance of \textit{TDLSTM} on the \textit{high} and \textit{low} \textit{TSSR} subsets are both above the text classification model unlike the other two TDSA models. Though this is a rather loose way of measuring sentiment overfitting and is not the way that was stated in the previous subsection \ref{section:aug_analysing_the_splits}. In the previous  subsection \ref{section:aug_analysing_the_splits} the difference between the \textit{high} and \textit{low} subsets was hypothesised to indicate sentiment overfitting, but as can be seen from the figures \textit{TDLSTM} that is supposed to not be overfitting as much as the other TDSA models does indeed contain a low difference between \textit{high} and \textit{low} on the Laptop dataset, but so does \textit{IAN} thus making the hypothesis less likely to be true. Therefore to conclude on the \textit{TSSR} split, it cannot measure sentiment overfitting nor would it be able to measure target interaction as suggested in \ref{section:aug_analysing_the_splits} either as it would be impossible to know if it was target interaction or sentiment overfitting. However there are clear signs that the subsets measure to some degree target sentiment relation as the score of subsets \textit{1}, \textit{high}, and \textit{low} are similar in order to subsets $DS_1$, $DS_2$, and $DS_3$ respectively and these subsets co-occur frequently as shown in figure \ref{fig:aug_error_analysis_tssr_ds_nt_breakdown}. Thus after this subsection the \textit{TSSR} split will no longer be used. 
    \item \textit{TSR} again the finding is expected where the \textit{USKT} is by far the most difficult subset. The \textit{UT} is more difficult in general than the \textit{KSKT} but with a much smaller margin. This finding is therefore in line with the relation extraction literature where unknown entities are easier to predict than unknown relations \citep{levy-etal-2017-zero,abdou-etal-2019-x}. Within the validation results for the Election dataset the margin between \textit{UT} and \textit{KTKS} is very small. This very much suggests that the models do require a certain amount of supervision for all targets in all sentiment classes or else they bias the target more towards one sentiment class than another. This type of bias can be very harmful as shown by the \textit{USKT}. This could suggest a reason why the margin between \textit{UT} and \textit{KSKT} is so small as some of the \textit{KSKT} targets might not occur in enough samples within a sentiment class. Furthermore the \textit{KSKT} subset can be seen as the upper limit for the other two subsets as it can be seen as the data rich subset.
    \item \textit{n-shot} the expected result can be clearly seen in all the datasets within the test split but less so within the validation split. Where the expectation is that the greater \textit{n} is the easier the subset will be. Within the validation split the Election and to some extent Restaurant datasets are the major outliers, where no matter what the subset is, the scores are almost all the same. A reason for this could be that the validation split is used in early stopping therefore some information is leaked to the model. As both the \textit{n-shot} and \textit{TSR} splits measure a model's generalisation to new targets, from the results shown it would appear that \textit{TSR} does this more explicitly. The \textit{TSR} split unlike the \textit{n-shot} models both the unseen targets and unseen relations, of which modelling both has been shown through \textit{TSR} to be crucial. This finding creates another possible explanation why the \textit{n-shot} subsets do not always show a positive correlation between \textit{n} and the metric score. Furthermore, the \textit{TSR KSKT} subset is always the best performing subset within the split unlike the \textit{high} in the \textit{n-shot}. Thus, for exploring a model's ability to generalise to unknown targets and unknown sentiment relations \textit{TSR} is recommended compared to \textit{n-shot}. Thus, like the \textit{TSSR} split the \textit{n-shot} will not be used after this subsection.
\end{itemize}

Generally, the test and validation results from figures \ref{fig:aug_baseline_test_error_subset} and \ref{fig:aug_baseline_validation_error_subset} respectively show that the \textit{DS}, \textit{TSSR}, and \textit{TSR} splits contain the most difficult subsets. The TDSA models perform a lot better on the $DS_2$ subset on the Election datasets compared to the Laptop and Restaurant datasets. This could be due to the Election dataset containing far more $DS_2$ samples relative to it's overall size compared to Laptop and Restaurant datasets (see figure \ref{fig:aug_error_analysis_ds}). This may suggest that ways to improve the models performance on the $DS_2$ and potentially $DS_3$ subsets could be by training the models on more of these samples and thus improving the target sentiment relation modelling. However, this could have a negative affect on the performance in the $DS_1$ subset. Also how to generate more $DS_2$ and $DS_3$ samples could also be a difficult and interesting challenge.

From the test and validation results in figures \ref{fig:aug_baseline_test_error_subset} and \ref{fig:aug_baseline_validation_error_subset} respectively the text classification model, has a few unexpected findings. The \textit{TSR}, and \textit{n-shot} splits do not explicitly probe a models capability to model the target sentiment relationship rather how well a model generalises to new targets or less seen targets and unknown sentiment classes for known targets. These probes thus do not explicitly require target information, for example in the \textit{DS} split for the $DS_2$ subset without modelling the target it is impossible to get all the samples correct, this is not directly true for the subsets in the \textit{n-shot} and \textit{TSR}. However, as can be seen from the results the text classification model does not perform equally well across all subsets in the \textit{n-shot} and \textit{TSR} splits, this suggest that either the text classification model does use the target to influence the sentiment prediction, or these subsets correlate with other dataset factors, for example \textit{zero-shot} subset has far fewer samples that belong to the $DS_2$ subset than the \textit{high-shot} subset. These issues are not explored any further but should be looked at in the future.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.45]{images/augmentation/methods_performance/baseline/test_error_subsets.pdf}
    \caption{The mean and standard deviation error bars for each error subset within all of the error splits on the test split across all datasets.}
    \label{fig:aug_baseline_test_error_subset}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.45]{images/augmentation/methods_performance/baseline/test_error_diff_subsets.pdf}
    \caption{The mean and standard deviation error bars for the difference between the overall accuracy and the accuracy from each error subset within all of the error splits on the test split across all datasets.}
    \label{fig:aug_baseline_test_error_diff_subset}
\end{figure}

\FloatBarrier
\subsubsection{Error Split Results Comparison between TDSA and Text Classification Models}

Using the test and validation results from the subsets shown in figures \ref{fig:aug_baseline_test_error_subset} and \ref{fig:aug_baseline_validation_error_subset} respectively it is possible to explore the differences between the text classification model and the TDSA models. These differences can be better seen through the heatmaps in figures \ref{fig:aug_baseline_dataset_error_subset_heatmap} and \ref{fig:aug_baseline_combined_dataset_error_subset_heatmap}, where the former is not corrected for multiple significance tests where as the later is using Bonferroni and is aggregated across datasets. Note that for figure \ref{fig:aug_baseline_dataset_error_subset_heatmap} the $DS_3$ subset results should be ignored for the Laptop and Restaurant datasets as they were never calculated as the sample size for the $DS_3$ subset is too small. Also the $DS_3$ subset is removed from figure \ref{fig:aug_baseline_combined_dataset_error_subset_heatmap} as only the Election dataset contains enough samples to create confidence scores. From all of these figures it is clear to see that the subsets that the TDSA models outperform the text classification model in are $DS_2$, $DS_3$, \textit{low-TSSR}, \textit{TSR KSKT}, \textit{n-shot Med}, and \textit{NT Low}. There are other subsets where the difference is significant as shown in the heatmaps but the majority of these significant differences only occur because of the Election dataset as can be seen if you compare figures \ref{fig:aug_baseline_dataset_error_subset_heatmap} and \ref{fig:aug_baseline_combined_dataset_error_subset_heatmap}. Furthermore, the outliers in these differences are the \textit{n-shot Med}, and \textit{NT Low} subsets of which the reason why it is believed these are outliers was described in the previous paragraphs. The $DS_2$, $DS_3$, and the \textit{low-TSSR} are expected to perform better for the TDSA models as they contain multiple unique sentiments within a sentence, for which a text classification model can only predict one of those sentiments for the sentence thus, limiting the model's capability to perform well on these subsets. This therefore shows that the TDSA models must be learning some target sentiment relationship modelling or else they would not be more competitive than the text classifier. The \textit{TSR KSKT} shows that when the TDSA models have seen a target enough times in a known sentiment context then they can perform a lot better than the text classification model and their respective overall accuracy. However it is the other subsets within \textit{TSR} that are of more interest showing the deficiencies of the TDSA models. The worse subset within \textit{TSR} is the \textit{USKT} of which this is the only subset where the text classification model in general perform significantly better (see figure \ref{fig:aug_baseline_combined_dataset_error_subset_heatmap}). TDSA models are most likely biasing the target representation towards a subset of sentiment classes for those targets and hence why the text classification models perform better on those targets. The \textit{UT} (\textit{n-shot zero} is the same subset) subset is an interesting result as it is dataset dependent as shown best in \ref{fig:aug_baseline_dataset_error_subset_heatmap}, in the Election dataset the TDSA models are better but in all other datasets the text classification model is better. This could be due to the size of the datasets as the Election dataset is much larger than the rest and therefore could allow the TDSA models to create better general target representations, thus allowing the models to leverage similarities with known targets. From the dataset heatmap figure \ref{fig:aug_baseline_dataset_error_subset_heatmap} it can be easily seen that on the Election dataset the majority of subsets are statistically significant compared to the Laptop and Restaurant dataset. This is most likely due to the Election dataset containing more targets per text (as can be seen in table \ref{table:augmentation_combined_dataset_statistics} \footnote{Dataset statistics for the splits, rather than the whole dataset, used in this section can be seen in table \ref{table:augmentation_splits_dataset_statistics}.}) and therefore far fewer texts within $DS_1$ which is the subset the text classification model is most suited to. Even though the text classifier does not perform  statistically significantly better than all the TDSA models on the Laptop and Restaurant datasets for $DS_1$ they are never worse. Furthermore, as the Laptop and Restaurant datasets are mainly made up of $DS_1$ samples (see figure \ref{fig:aug_error_analysis_ds}) this is most likely the reason why the TDSA models are not statistically significantly better than the text classification models on these datasets.



\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.45]{images/augmentation/methods_performance/baseline/baseline_dataset_error_subset_heatmap.pdf}
    \caption{Plots in the first column represent the number of TDSA models that are statistically significantly better than the text classification model. Plots in the second column show the opposite, the number of TDSA models where the text classification is statistically significantly better. All plots have a confidence level of 95\% ($p \leq 0.05$).}
    \label{fig:aug_baseline_dataset_error_subset_heatmap}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.5]{images/augmentation/methods_performance/baseline/baseline_combined_dataset_error_subset_heatmap.pdf}
    \caption{Plots in the first column represent the number of TDSA models that are statistically significantly better than the text classification model across all datasets. Plots in the second column show the opposite, the number of TDSA models where the text classification is statistically significantly better. All plots have a confidence level of 95\% ($p \leq 0.05$) and have been corrected using Bonferroni.}
    \label{fig:aug_baseline_combined_dataset_error_subset_heatmap}
\end{figure}


\afterpage{
    \begin{landscape}% Landscape page
            \centering
            \input{tables/augmentation/methods_performance/Baselines_results/combined_dataset_stats}
    \end{landscape}
}

\newpage
\FloatBarrier
\subsubsection{Comparing the Error Split Results to the Prior Work}
\label{section:augmentation_cwr_comparing_to_prior_work_new}
% How does all of this relate back to the previous work this needs to be moved up to above the NT stuff.
From the results in this subsection the findings can relate back to some of the original work on these splits confirming the same findings. The findings of the \textit{TSSR 1-Multi} performing better than \textit{TSSR 1} is the same finding as \citet{nguyen-shirai-2015-phrasernn} as both of these subsets are the equivalent to \textit{ST2} and \textit{ST1}. When the number of unique sentiments increase in a text, which can be measured through the \textit{DS} and \textit{TSSR} splits, this reduces the performance of a method, which is the same finding as \citet{xue-li-2018-aspect} comparing the normal test to the \textit{hard} test, and that of \citet{nguyen-shirai-2015-phrasernn} comparing \textit{ST1} or \textit{ST2} with \textit{ST3}.  

The \textit{n-shot} findings do not confirm the findings of the original work by \citet{yang2018multi} where they found that in general models performance do not correlate with the number of times the target/aspect appeared in the training data. However, our findings show that they do correlate where the more the target appears in the training data the better the performance in general. The reason for the difference could come from the task itself, as \citet{yang2018multi} was not solving the task of TDSA but rather the task of Multi-Entity Aspect Based Sentiment Analysis (ME-ABSA), where TDSA would be equivalent if when predicting the sentiment of the target the latent aspect was also given. This difference in task could make a large difference as knowing a target's latent aspect could greatly improve a model's performance on unknown targets. The reason why the latent aspect would make such a large difference is because the dataset would contain a few aspects which occur frequently therefore allowing the model to create a good representation for the aspects. Furthermore, given these aspects the likelihood is that there could be many unknown aspect target pairs but due to the model potentially having a good representation of the aspect the performance on these \textit{zero-shot} pairs could be quite high. Thus, a reason for \citet{yang2018multi} finding no correlation between performance and number of times the target/aspect appeared in the training data could be due to the aspects.

%
% The above three paramgraphs need to be merged with the one below. as need to add a paragraph on DS
%
% The \textit{NT} split by \citet{zhang-etal-2019-aspect} which was found to be unstable in that original work and similar in this thesis, was to some extent explained empirically why it was for the first time in this thesis. The reason in \citet{zhang-etal-2019-aspect} work to overcome the stability was to better model inter target dependencies (target interaction). However considering the empirical evidence put forward here as shown in figures \ref{fig:aug_baseline_ds_nt_test_scores} and \ref{fig:aug_baseline_tssr_nt_test_scores}, the belief is that the \textit{NT} split itself does not measure target interaction and a dataset that does so would be a more suitable suggestion.

As found earlier in figures \ref{fig:aug_baseline_test_error_subset} and \ref{fig:aug_baseline_validation_error_subset} for the test and validation results respectively the \textit{NT} split does not show any consistent trend, which to some degree is what is found in the original works \citep{he-etal-2018-effective,zhang-etal-2019-aspect}. The expected trend was as the number of targets increase the lower the performance. A potential reason for this could be that there are other factors that influence the \textit{NT} split. The factors that will be explored here are the target sentiment relationship factors which can be measured to some extent using the \textit{DS} and \textit{TSSR} splits. To explore this all the datasets will be first subsetted by one of the \textit{DS} or \textit{TSSR} subsets and then further subsetted by one of the \textit{NT} subsets, the model's performance will be measured on each one of these compounded subsets. Figures \ref{fig:aug_baseline_ds_nt_test_scores} and \ref{fig:aug_baseline_tssr_nt_test_scores} show the performance on these compounded subsets whereby the former subsets the data by \textit{DS} and the latter \textit{TSSR}, for the validation data this can be seen in figures \ref{fig:aug_baseline_ds_nt_validation_scores} and \ref{fig:aug_baseline_tssr_nt_validation_scores}. Note that in the figures some of the \textit{NT} subsets do not exist on the x-axis, this is because after the subset compounding no data exists for those subsets. These figures show that in general for the $DS_1$ and \textit{TSSR 1-Multi} rows the larger \textit{NT} the better the performance of the models, of which for the $DS_1$ row this can be better seen in the validation data (figure \ref{fig:aug_baseline_ds_nt_validation_scores}) than the test. This is most likely the case because of the models exploiting the fact that there are more targets expressing the same sentiment. This exploitation of targets expressing the same sentiment can also be seen in the $DS_2$ rows (better seen in the validation data) where the more targets the better the score. Within the \textit{Low} and \textit{High TSSR} subsets the trend is less clear. The expectation within these subsets would be, when there are more targets (larger \textit{NT}) this will results in poorer performance for the \textit{Low TSSR} subset, but better results for the \textit{High TSSR} subset. This expectation is under the assumption that the more targets there are within the \textit{High TSSR} subset the greater the likelihood that the targets have the same sentiment and exploiting the most frequent sentiment would gain a higher performance score. The opposite is true when there are more targets within the \textit{Low TSSR} subset the greater the likelihood that the targets have a different sentiment and exploiting the most frequent sentiment would gain a lower performance. However, this expectation is not always true and can be inconsistent between splits, for example the \textit{High TSSR} Laptop results have different trends between test and validation splits. Furthermore, this assumption of more targets within the \textit{TSSR Low} and \textit{High} subsets does not necessarily mean more targets of the most frequent sentiment class due to the way \textit{TSSR} subsets are created (equation \ref{eq:aug_tssr}), hence a potential reason why there is no consistent correlations in those subsets. Thus, this analysis shows to some extent why the \textit{NT} split has no trend as the target sentiment relationship factors are more influential than the number of targets on the performance of the models. This therefore solves to some extent why \citet{zhang-etal-2019-aspect}\footnote{See figure 4 in the paper.} and \citet{he-etal-2018-effective}\footnote{See figure 3 in the paper.} also could not find a steady trend for the \textit{NT} split.
%The $DS_2$, $DS_3$, \textit{High TSSR}, and \textit{Low TSSR} rows are less clear most likely due to the $DS_2$ and $DS_3$ containing data that can be in both \textit{Low} and \textit{High} \textit{TSSR} subsets. Thus as stated earlier the model could be overfitting to one sentiment in each sentence and hence why there are trends in the \textit{Low (High) TSSR} subset that when there are more (less) targets (higher \textit{NT}) the worse the results. This analysis shows that there are trends in the \textit{NT} subsets when sentiment relation factors have been taken into account as shown by subsetting the data first by one of the \textit{TSSR} or \textit{DS} subsets. Thus this analysis shows to some extent why the \textit{NT} split has no trend as the target sentiment relationship factors are more influential than the number of targets on the performance of the models. This therefore solves to some extent why \citet{zhang-etal-2019-aspect}\footnote{See figure 4 in the paper.} also could not find a steady trend for the \textit{NT} split. 

%This analysis however brings up the question of how to measure target interaction explicitly as the \textit{NT} split clearly cannot do this. Thus a dataset that has target interaction annotated would be of use to the community to better understand these models. However careful consideration will have to be taken when creating such a dataset so that other factors are not being measured at the same time. One could design such a dataset so that the only way a target can be predicted is through target to target interaction, and not possible via target sentiment relationship or most frequent sentiment class within the sentence. Thus it can be concluded that the \textit{NT} split is not fit for any purpose therefore will not be used after this subsection.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.42]{images/augmentation/methods_performance/baseline/baseline_ds_nt_test_scores.pdf}
    \caption{Each plot shows the performance (y-axis accuracy) of the given models and sample size of the data evaluated on (y-axis dataset size) on the different test datasets (columns) after being subsetted by the relevant \textit{DS} subset (rows) and then NT subset (x-axis).}
    \label{fig:aug_baseline_ds_nt_test_scores}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.42]{images/augmentation/methods_performance/baseline/baseline_tssr_nt_test_scores.pdf}
    \caption{Each plot shows the performance (y-axis accuracy) of the given models and sample size of the data evaluated on (y-axis dataset size) on the different test datasets (columns) after being subsetted by the relevant \textit{TSSR} subset (rows) and then NT subset (x-axis).}
    \label{fig:aug_baseline_tssr_nt_test_scores}
\end{figure}

The $DS_i$ split was stated to get more difficult as $i$ increased and this has been shown in this work and in the original \citep{wang-etal-2017-tdparse}. However, as mentioned in section \ref{section:aug_error_analysis_previous_work}, in the original work it was also shown for some methods and metrics that the models perform best on the $DS_3$ subset. In this work that phenomena did not occur when using the accuracy metric, which \citet{wang-etal-2017-tdparse} did not use. Therefore, to test if the \textit{DS} split results do change because of the metric, in figure \ref{fig:aug_baseline_sentiment_f1_ds_all} are the \textit{DS} results on all datasets using the macro F1 metric which was one of the metrics used by \citet{wang-etal-2017-tdparse}. As can be seen from the results the only dataset where $i$ in $DS_i$ does not negatively correlate with the macro F1 results is the Election test dataset. The Election test datasets was also the only dataset \citet{wang-etal-2017-tdparse} used when measuring model performance on the \textit{DS} split\footnote{Results can be found in table 4 of \citet{wang-etal-2017-tdparse}.}. The other results follow the trend shown in this section when using the accuracy metric. 

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{images/augmentation/methods_performance/baseline/sentiment_f1_ds_all.pdf}
    \caption{Macro F1 score where the data is subsetted by the \textit{DS} split.}
    \label{fig:aug_baseline_sentiment_f1_ds_all}
\end{figure}

To investigate why the Election test dataset does have a different trend when using a different metric the approach taken was to explore the individual F1 scores for each sentiment label on the Election dataset. This approach was taken as the main difference between accuracy and macro F1, as the macro F1 is not biased by the un-balanced label distribution that is within the dataset, of which all datasets used are very un-balanced (see table \ref{table:augmentation_combined_dataset_statistics}). Figure \ref{fig:aug_baseline_sentiment_f1_ds_election} shows the results of each sentiment label's F1 score and as expected the most frequent sentiment (negative) has the highest scores no matter the subset. These results highlight why the macro F1 score does not have the same negative correlation as the accuracy metric does for the \textit{DS} split. As the results show the positive and the neutral sentiments do not follow the negative correlation that the negative sentiment does. Thus, as the scores of each sentiment are weighted the same in the macro F1 metric this therefore causes the macro F1 score for the $DS_3$ subset to be higher than the $DS_2$ subset in the test split. The potential reason for this unusual correlation could be due to the model overfitting to the most frequent sentiment class (negative) and hence why if the model predicted negative for all samples in a $DS_3$ sentence then it would get some samples correct but it would get at least 2 samples wrong.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{images/augmentation/methods_performance/baseline/sentiment_f1_ds_election.pdf}
    \caption{The Election test and validation split F1 scores for each sentiment label, where the data has been further broken down through \textit{DS} subsets.}
    \label{fig:aug_baseline_sentiment_f1_ds_election}
\end{figure}

To further investigate whether the most frequent sentiment class always has this negative correlation the results for the Restaurant and Laptop datasets are shown in figures \ref{fig:aug_baseline_sentiment_f1_ds_restaurant} and \ref{fig:aug_baseline_sentiment_f1_ds_laptop} respectively. From these two plots we can see that the most frequent sentiment class (positive for both datasets) has the largest drop in F1 score from $DS_1$ to $DS_2$. This large drop in F1 score gives some extra merit to the idea that the models are overfitting to the most frequent sentiment class\footnote{It was also found in chapter \ref{chapter:reproducibility} section \ref{section:repro_mass_eval} that NN based methods overfit to the most frequent sentiment class on many datasets in a low resource setting.}. Due to this overfitting the model is most likely predicting the most frequent sentiment class more often than it should where as in the cases for the least frequent sentiment classes it could be only predicting these when it is confident. These reasons are not empirically proven but the results have shown further insight into the \textit{DS} split. Lastly, these results show more that the results from the original paper \citep{wang-etal-2017-tdparse} do not generalise across datasets and that the general result is that the metrics normally correlate negatively with the \textit{DS} subsets. 

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{images/augmentation/methods_performance/baseline/sentiment_f1_ds_restaurant.pdf}
    \caption{The Restaurant test and validation split F1 scores for each sentiment label, where the data has been further broken down through \textit{DS} subsets.}
    \label{fig:aug_baseline_sentiment_f1_ds_restaurant}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{images/augmentation/methods_performance/baseline/sentiment_f1_ds_laptop.pdf}
    \caption{The Laptop test and validation split F1 scores for each sentiment label, where the data has been further broken down through \textit{DS} subsets.}
    \label{fig:aug_baseline_sentiment_f1_ds_laptop}
\end{figure}

\FloatBarrier
\subsection{The Strict Text ACcuracy (STAC) Metric}
\label{section:augmentation_cwr_STAC_new}

Both the \textit{DS} and \textit{TSSR} error splits explore the concepts of target sentiment relationships and overfitting to the most common sentiment within a text. However, neither of these can create error subsets that will explicitly inform you if the model can detect sentiment for all the targets in the text and thus performing the target sentiment relationship task perfectly. Both the \textit{DS} and \textit{TSSR} splits do attempt to show this but both are subject to the model finding the most frequent or easiest to find sentiment for some/all of the targets in the subsets. Thus, the creation of the Strict Text ACcuracy (\textit{STAC}) metric. This metric works on the sentence/text level compared to the accuracy and macro F1 metrics that are based at the target level. \textit{STAC} treats each sentence as a sample and each sentence can only be correct if all targets within that sentence have been classified correctly, it is then averaged by the number of sentences. The \textit{TAC} equation \ref{eq:aug_tac} that is used within the \textit{STAC} equation \ref{eq:aug_stac} finds the average number of targets that are correct within a sentence. The notation to describe \textit{STAC} and \textit{TAC} in equations \ref{eq:aug_stac} and \ref{eq:aug_tac} is the same notation used in equation \ref{eq:aug_tssr}, which describes the \textit{TSSR} split. $T_j$ within \textit{STAC} represents all of the targets within sentence $j$ from all sentences $X$ that is within the dataset, and $t_{ji}$ represents target $i$ true sentiment within sentence $T_j$ where $\hat{t_{ji}}$ is the predicted sentiment.

\begin{equation}
    \text{Text ACcuracy (TAC)}(T_j) = \frac{\sum_{i=1}^{|T_j|} [t_{ji}=\hat{t_{ji}}]}{|T_j|}
    \label{eq:aug_tac}
\end{equation}
\begin{equation}
    \text{Strict Text ACcuracy (STAC)} = \frac{\sum_{j=1}^{|X|} \begin{cases}
    1,& \text{if } \text{TAC}(T_j) = 1\\
    0,              & \text{otherwise}
\end{cases}}{|X|}
\label{eq:aug_stac}
\end{equation}

The \textit{STAC} metric is more useful when applied to subsets of a dataset, thus two specific versions of the \textit{STAC} metric are created:
\begin{enumerate}
    \item \textit{STAC 1} - The \textit{STAC} metric applied to only the data in the $DS_1$ subset.
    \item \textit{STAC Multi} - The \textit{STAC} metric applied to only the data in the $DS_2$ and $DS_3$ subsets.
\end{enumerate}

The \textit{STAC Multi} gives in one metric how well overall a TDSA model is at target sentiment relation modelling removing all factors of overfitting to a sentiment class, or predicting the most frequent sentiment, of which this is possible in \textit{STAC 1}. \textit{STAC Multi} can also be seen as a coarse grained and much stricter version of the \textit{DS} split, as both measure target sentiment relationship modelling. However, due to \textit{STAC Multi} being such a strict and thus difficult metric the \textit{DS} subsets can be useful to measure target sentiment relationship modelling at a more fine grained scale. For example, if a model does not perform significantly better nor worse than another on \textit{STAC Multi} but does perform better on $DS_2$ and $DS_3$ subsets, then the likelihood is that the model is performing target sentiment relationship modelling better. The difference between \textit{STAC 1} and \textit{STAC Multi} can show to some degree how much the model is overfitting to the most frequent sentiment class in a text. The performance of all models across all metrics including accuracy and macro F1 can be seen in \ref{fig:baseline_stac_scores.png}, the \textit{STAC} metric is shown for completeness. As can be seen from the figure the \textit{STAC Multi} is by far the most difficult metric and scores much lower than any of the accuracy metrics on any of the subsets shown in figure \ref{fig:aug_baseline_test_error_subset} (validation split figure \ref{fig:aug_baseline_validation_error_subset}). However, the \textit{STAC 1} results can be the easiest metric as shown by the Restaurant dataset. The difference between \textit{STAC 1} and \textit{STAC Multi} for the TDSA models is rather large and more so for the Restaurant and Laptop datasets which could be due to the fact there are proportionally and overall more $DS_2$ and $DS_3$ sentences in Election than the Restaurant and Laptop datasets as shown by table \ref{tab:aug_STAC_samples_stats}. Furthermore, as should be the case, the text classification model (\textit{CNN}) scores $0$ in all of the \textit{STAC Multi} thus showing again the point of the metric and the relevancy to TDSA. These scores highlight that TDSA models have much to improve upon with regards to target sentiment relation modelling as shown by the \textit{STAC Multi} metric without resorting to simpler majority sentiment classification of the sentence as shown by \textit{STAC 1}, and the other error subsets ($DS_1$, \textit{TSSR 1}, and \textit{TSSR 1-Multi}). Furthermore, from the results it is interesting to see that the \textit{TDLSTM} model generally performs well on the \textit{STAC Multi} metric compared to the other models across all datasets, of which this could be due to the model encoding position of the target within its architecture. 

\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.4]{images/augmentation/methods_performance/baseline/baseline_stac_scores.pdf}
    \caption{Performance across all metrics for all models across all datasets and splits.}
    \label{fig:baseline_stac_scores.png}
\end{figure}

\begin{table}[ht!]
    \centering
    \input{tables/augmentation/methods_performance/Baselines_results/baseline_STAC_samples_stats.tex}
    \caption{Number of sentences in each split for all datasets.}
    \label{tab:aug_STAC_samples_stats}
\end{table}

\FloatBarrier
\section{Reflection on Error Splits, STAC, and the Results}
Through these error splits and new metrics (\textit{STAC}) the differences between the TDSA and text classifier can be seen and where the TDSA models do outperform the text classifiers by a large margin. The flip side to distinguishing the performance of TDSA and text classifier models is by creating `challenge datasets' that examine the performance of a model in specific circumstances. This approach was taken by \citet{jiang-etal-2019-challenge} where they created a new version of the Restaurant dataset called Multi-Aspect Multi-Sentiment (MAMS). The MAMS dataset as the name suggests only contain texts that have at least two targets with at least two different sentiments, thus removing all texts that only have one sentiment. This new dataset was created to avoid samples being easily classified by a text level classifier. This dataset therefore fits into the $DS_2$ and $DS_3$ only subsets from the \textit{DS} split. They found a large difference in scores between the text classifier models and the TDSA ($\geq 10\%$), which is what was found in the error split analysis in section \ref{section:augmentation_cwr_error_split_results_sub} on all datasets as shown in figures \ref{fig:aug_baseline_test_error_subset} and \ref{fig:aug_baseline_test_error_diff_subset}. However, to overcome this problem they have had to create a new dataset which costs in either money and/or time, which is not the case in the error split approach shown here. The approach of creating new datasets to examine properties of TDSA models is not scalable without large resources thus the error split approach is a very feasible alternative and as shown effective. Furthermore, using the new \textit{STAC-Multi} metric it is now possible to quantify TDSA model performances on samples that only TDSA models can correctly predict. This does not mean that these challenge datasets are not useful, for instance using this dataset can help answer the question of whether using more $DS_2$ and $DS_3$ samples will improve TDSA models performance on those subsets and how much would that affect the performance on the $DS_1$ subset.

The baseline experiments (section \ref{section:aug_baseline}) have brought about many different findings, of which some have confirmed prior findings, where as others have not. Unlike previous work it has been shown that TDSA models on datasets that do not contain a lot of targets per text such as the Laptop and Restaurant datasets can be statistically no better than a text classifier, prior work has shown this in absolute performance but not statistically \citep{jiang-etal-2019-challenge}. From these baseline experiments we can conclude that all the error splits have been successfully tested across a range of models and datasets. From analysing the error split results the baseline TDSA models generally perform best on subsets of data that contain one unique sentiment ($DS_1$) and on targets that appear multiple times in different sentiment classes within the training data (\textit{KSKT}). This finding suggests that the baseline models are very brittle and cannot generalise to unknown targets (\textit{UT}), unknown sentiment relations (\textit{USKT}) or texts that contain multiple unique sentiments ($DS_2$ and $DS_3$). Due to these factors the models are unlikely to perform well in low resourced or cross domain settings. Subsection \ref{section:augmentation_cwr_STAC_new} introduced a novel TDSA metric \textit{STAC} and its two variants \textit{STAC-Multi} and \textit{STAC 1} of which when used together can show sentiment overfitting to the most frequent sentiment class in a text to some extent. Furthermore the \textit{STAC-Multi} shows how well the TDSA models can perform target sentiment relationship modelling perfectly, as well as how the \textit{DS} subsets are a fine grained and easier version of \textit{STAC Multi}.

Subsection \ref{section:augmentation_cwr_comparing_to_prior_work_new} has related back to the original work that created these error splits in doing so explained why the \textit{NT} split does not have a consistent trend. From exploring the different splits many of them have been dismissed due to the results not matching the hypothesis of what the split is supposed to measure. Thus the \textit{NT} split, due to having no consistent trend, cannot be used to measure target interaction. The \textit{TSSR} split cannot measure sentiment overfitting, and the \textit{n-shot} split is not as useful as the \textit{TSR}. Therefore the recommended splits to use are the \textit{DS} for measuring fine grained target sentiment relation modelling and \textit{TSR} to measure the model's ability to generalise to unseen targets and sentiment relationships. Furthermore, the \textit{STAC-Multi} metric is recommended to measure target sentiment relation modelling, but it is a much stricter and coarser measure compared to \textit{DS}. The \textit{STAC 1} should also be used so that it can show to some degree with \textit{STAC-Multi} the extent of overfitting to the most frequent sentiment class in a sentence.

Lastly, from exploring the results across these different splits future research directions have surfaced. Due to none of the splits being capable of explicitly measuring target interaction, an annotated corpus incorporating this annotation would be of use. \citet{he-etal-2018-effective} has shown that using an un-supervised autoencoder objective to mimic encoding a latent aspect into the target representations improves general results as well as results on multi word targets, and visually has shown on selected targets to create better target representations. However it would be of interest to see if such a method can help target representations for unknown targets (\textit{UT}) as this would be similar to the Multi-Entity Aspect Based Sentiment Analysis (ME-ABSA) task, where \citet{yang2018multi} had found no difference between \textit{UT} and Known Sentiment Known Targets (\textit{KSKT}). Thus suggesting that encoding the latent aspect could greatly benefit the \textit{UT} samples.

\FloatBarrier
\section{Conclusion}
The research question that this chapter was attempting to answer is \rqref{rq:measured} `What is an appropriate empirical evaluation methodology for TDSA?'. To investigate this, section \ref{section:aug_error_analysis} reviewed the prior work in error analysis splits within TDSA. From this literature review, several existing error splits were found, \textit{DS} which measured target sentiment relationship modelling, \textit{NT} measuring target interaction, and \textit{n-shot} measuring generalisation to unknown targets. From this literature review, two novel error splits were created, \textit{TSSR} that measured target sentiment overfitting to the most frequent sentiment in a sentence and \textit{TSR} measuring generalisation to unknown sentiment relationships and targets. These existing error splits were rigorously tested in section \ref{section:aug_baseline} across three TDSA methods and a text classification method to ensure they were measuring what was hypothesised. From this, \textit{NT} error split was removed due to it not measuring target interaction but rather the sentiment factors \textit{DS}. \textit{TSSR} was dropped due to it not measuring target sentiment overfitting without a text classification model and the \textit{DS} split. Lastly, the \textit{n-shot} split was removed as when the value of \textit{n} increased it was expected the accuracy should increase as well or at the least not drop, which was not always true. Thus the \textit{TSR} split which measured both unknown targets and sentiment relationships was recommended as a better replacement to \textit{n-shot}. The findings from reviewing the \textit{NT} split bring the recommendation that the only way to investigate target interaction is through an annotated corpus with this explicitly annotated. A novel TDSA metric is created, \textit{STAC Multi} and \textit{STAC 1}, which when used together can be used to evaluate sentiment overfitting to the most frequent sentiment in the sentence. Furthermore, the \textit{STAC Multi} metric can be seen as a coarse grained version of the \textit{DS} error split as they both measure target sentiment relationship modelling, but \textit{STAC Multi} cannot be influenced by sentiment overfitting to the most frequent sentiment in the sentence. 

In this chapter the error splits have been reduced to those that match their hypotheses (\textit{DS} and \textit{TSR}) and a new novel metric has been created to overcome previous limitations in the error splits. Therefore a new empirical evaluation methodology for TDSA has been created, whereby each error split and metric can be used to quantify different theories about a TDSA method.





%3. Even though TSSR was mainly created to measure sentiment overfitting, through differences in the High and Low subset scores, this overfitting measure is a somewhat more fine grained version from the STAC metric. In the STAC metric the gap between STAC Multi and STAC 1 is normally large and this can be due to overfitting, but is more likely due to the model getting just one prediction wrong, hence why the gap between High and Low is much smaller. Thus when the sample size is large enough in the High and Low TSSR subsets this is another good measure of overfitting to the most frequent sentiment class without the requirement of having to predict all targets correctly. 4. The DS subsets show on a discrete scale the extent to which the TDSA models can detect sentiment relations, where the better it is at the top end of the scale DS3 the more likely the model is performing sentiment relation extraction on each target. This can be seen as a more fine grained analysis on the STAC Multi metric where this metric is an extremely difficult version of the DS3 and DS2 subsets. Furthermore this can empirically be seen in the analysis, where the TDLSTM model performs best in the STAC Multi metric and also performs in the top 2 of the DS3 subset on the Election dataset in both splits compared to the ATT-AE which performs worse in the TDSA models in both DS3 and STAC Multi.5. The n-shot subset can measure to some extent the ability of a model to generalise to new targets. However the TSR subsets results shows this more explicitly due to the subsetting the data based on both the target occurrence and the occurrence of the target with a sentiment class in the training data. Due to this it can be seen that even if a target is known it can perform badly on that target if the sentiment associated to it has not be seen before in the training data. This finding can possibly explain why the n-shot subsets do not show a positive correlation between n and the metric score. Furthermore the TSR KSKT subset is always the best performing subset within the split unlike the high in the n-shot. Thus for exploring a models ability to generalise to unknown targets and unknown sentiment relations TSR is recommended compared to n-shot as both of these subsets can be compared to KSKT subset as the upper limit as KSKT can be seen as the data rich subset.
