\FloatBarrier
\section{Thesis Summary}
This thesis, in chapter \ref{chapter:lit_review}, has summarised the literature related to sentiment analysis from a coarse to fine grained perspective, in doing so has created one of, if not, the most extensive reviews of sentiment analysis applied to the English language, that links the different granularities together. In the review a new extended definition of fine grained sentiment analysis has been created, the hextuple, which in comparison to the original \citep{liu2015sentiment} removes sentiment ambiguity that was shown through multiple examples and empirical evidence from two existing datasets. Further, parts of the original definition by \citet{liu2015sentiment} that is used within the hextuple, time and sentiment holder, are justified for the first time in this thesis through sentiment ambiguity rather than application uses \citep{liu2015sentiment}. The literature review concludes with multiple further related topics on implicit sentiment, discourse level considerations, and stance detection, the first two related topics will be revisited in the future work section \ref{section:conclusion_future_work} whereas the stance detection topic discussed the similarities between it and sentiment analysis.

The first reproduction study for TDSA was conducted in chapter \ref{chapter:reproducibility}, in which all three chosen papers were successfully reproduced. From these studies it was found for the two NP methods \citep{vo2015target, wang-etal-2017-tdparse} that scaling features and the C-value for the SVM classifier are significant factors in these methods, and it was noted that \citet{vo2015target} never reported scaling the features and \citet{wang-etal-2017-tdparse} did not report the C-value they used. It was further tested that both scaling features and the C-value are not just significant factors for the methods on the one (two) dataset(s) that \citet{vo2015target} (\citet{wang-etal-2017-tdparse}) used, but is shown to be significant across six diverse datasets\footnote{These six datasets include the datasets from \citet{vo2015target} and \citet{wang-etal-2017-tdparse}.}. Thus it was concluded that both scaling and the C-value used should be reported within a structured format like that suggested by \citet{dodge-etal-2019-show} within Appendix B-D. For the LSTM methods it was shown for at least one metric (macro F1) that they can be statistically significantly affected by random seeds, which is what \citet{reimers-gurevych-2017-reporting} found for neural sequence labelling methods. These findings suggests a possible reason why previous papers that attempted to replicate \citep{chen-etal-2017-recurrent} and reproduce \citep{tay2018learning} \citet{tang-etal-2016-effective} methods could not do so, this problem was the original motivation behind the reproduction of \citet{tang-etal-2016-effective}. From this it was concluded to follow \citet{reimers-gurevych-2017-reporting} advice on reporting the distribution of results from multiple runs to allow for better evaluative comparisons and more accurate representation of the method, as single run scores can be misleading.

Additionally, it was found for \citet{vo2015target} methods that larger general word embeddings (GloVe) are at least as good and in \citet{tang-etal-2016-effective} case were found to be in general better than smaller type and/or task specific embeddings. This suggests that for \citet{vo2015target} methods that once the smaller type and/or task specific embeddings have been trained the methods can be more energy efficient without losing any performance gains. Alternatively it can be seen that smaller type and/or task specific embeddings are not required. These findings are rather restricted findings as they were only tested on the one Twitter dataset \citep{dong-etal-2014-adaptive}. These findings from the reproduction studies thus answers \rqref{rq:lessons} `What lessons can be learned from reproducing a method within TDSA?'.

Additionally, in chapter \ref{chapter:reproducibility}, the methods reproduced from the chosen studies are then used within the mass evaluation experiments to test the methods' generalisable capabilities. These three diverse sets of methods are then evaluated across six English datasets that differ in type, domain, medium, dataset sizes, and sentiment class distributions. It was found that no one method performs best or could be declared generalisable, but the NP methods performed better across more datasets than the LSTM methods, however the LSTM methods tend to prefer larger datasets. The LSTM methods were shown to be greatly affected by dataset size and/or sentiment class distribution and when in the low resource setting for all datasets the LSTM methods in some cases can only predict the majority class correctly. Thus it was found that the NP methods are the preferred method in low resource or highly unbalanced class distribution settings. When in the low resource setting it was found that sentiment lexicons, for the NP methods, as an inductive bias, can be useful if the lexicon comes from the same type and medium. However when the NP methods were tested in the normal settings the sentiment lexicons were not shown to be useful, this was also found for the dependency parser features in both low and normal settings. By testing the methods across these six datasets it showed that the novel target aware LSTM methods (TDLSTM and TCLSTM) could be beaten by the non-target aware baseline on at least one dataset, and this was never found in the original work \citep{tang-etal-2016-effective}, as they only evaluated on one dataset. All of these findings from the mass evaluation experiments answer \rqref{rq:generalisable} `How generalisable are existing methods within TDSA?'.

In chapter \ref{chapter:methodology} the prior work on error analysis splits were reviewed extensively, from which two new error splits are created: \textit{TSSR} which measures sentiment overfitting to the most frequent sentiment class within the text, and \textit{TSR} which measures generalisation to Unknown Targets (\textit{UT}s) and Unknown Sentiment relationships for Known Targets (\textit{USKT}s). The prior and new error splits are then analysed across three English datasets showcasing the differences between the datasets through the splits, through which it is found that smaller datasets tend to have more \textit{UT}s and \textit{USKT}s. These findings thus show that in the low resource setting a method that can generalise well to \textit{UT} and \textit{USKT} is required to perform well. Lastly, these prior and new splits are summarised describing what they do and what they hypothetically measure. Three different Neural Network (NN) based TDSA methods and one baseline non-target aware NN method are compared across three English datasets and the results are analysed using the splits. From the analysis it is found that the \textit{NT} split does not measure target interaction, which it was originally hypothesised to measure \citep{zhang-etal-2019-aspect}. Further, it is recommended to use the \textit{TSR} split over a prior works' \textit{n-shot} split due to it measuring the difference between known targets and \textit{UT} better as well as being capable of also measuring \textit{USKT}s. Also the \textit{TSSR} split could not identify sentiment overfitting to the most frequent sentiment class, however a new metric \textit{STAC} was created whereby the metric difference between its two variants \textit{STAC 1} and \textit{STAC Multi} is hypothesised to better measure the sentiment overfitting. The \textit{DS} split was shown to increase in difficulty when more unique sentiments existed within the text, and thus believed to measure target sentiment relationship modelling. However the \textit{DS} split can be influenced by the most frequent sentiment class within the text. Thus the \textit{STAC Multi} metric can be used as a coarse grained version of \textit{DS} whereby this measure is not influenced by the most frequent sentiment class within the text. Therefore this chapter concludes with a set of recommended error splits and metrics that can measure different phenomena within TDSA, forming the new empirical evaluation methodology for TDSA. Thus the chapter answers \rqref{rq:measured} `What is an appropriate empirical evaluation methodology for TDSA?'

Following the creation of this new empirical evaluation methodology within chapter \ref{chapter:methodology}, chapter \ref{chapter:case_study_methodology} conducts several case studies to further test the evaluation methodology. These case studies use the same methods and datasets as chapter \ref{chapter:methodology}, but each case study enhances the method with a new development. These enhanced methods are then compared to their non-enhanced version using the new evaluation methodology. The findings from these case studies are then compared, where appropriate, to the original hypothesis that were associated with the relevant development. The original hypothesis from the prior works have only ever been justified either through small qualitative case studies, improvements in overall metric scores, such as accuracy on the entire datasets, or through the unsuitable \textit{NT} error split as shown in chapter \ref{chapter:methodology}. From the three case studies the position encoding enhancement was shown to match the original hypothesis of improving target sentiment relationship modelling through significant improvements on the \textit{STAC Multi} metric and $DS_2$ subset. A negative result was found through the inter-target encoding enhancement, whereby the non-enhanced version was no worse and at times better than the enhanced one. Further it would not have been possible to measure the original hypothesis of inter-target encoding as the empirical evaluation methodology cannot measure target interaction. Additionally it is believed that the negative results might be due to the original work by \citet{hazarika-etal-2018-modeling} not evaluating their methods rigorously enough, as they only report results from one run, and it has already been shown in chapter \ref{chapter:reproducibility} that results from different runs can be significantly different. The last case study evaluated the CWR enhancement, where no prior work has shown how they improve over non-CWR apart from through overall metrics like accuracy. The CWR significantly performed better on the \textit{UT} and \textit{USKT} and this was not found for any of the other developments. Additionally, significant improvement also occurred for the \textit{STAC Multi} metric and $DS_2$ samples suggesting that it improves target sentiment relationship modelling, but these results were more convincing on the Election dataset which contained higher quantities of $DS_2$ and $DS_3$ samples. This suggests in part that the CWR enhancement might be overfitting to artefacts within the trained dataset. These case studies have rigorously tested the new empirical evaluation methodology and demonstrated how it can better quantify new developments and how these new developments improve TDSA. 

Throughout this thesis each experiment has been conducted within a rigorous experimental setup, which includes significance testing and using a correction procedure, Bonferroni, when appropriate.

\section{Research Limitations}
Given that the thesis summary has stated how the three research questions have been addressed within this thesis, this section will state the limitations of the answers to these research questions. 

\begin{itemize}
    \item \textbf{What lessons can be learned from reproducing a method within TDSA?} (\rqref{rq:lessons})\newline
    The answer to this has only tested for two parameters for the NP methods, scaling features and C-values within the SVM, and one parameter, random seeds, within the neural LSTM methods. There are many more parameters that could have been tested, for instance not having a standard train, validation, test setup for the neural methods and only having a training and test setup. It has already been shown that there can be differences between methods when using different train and test splits as shown for POS tagging \citep{gorman-bedrick-2019-need} and NER \citep{moss-etal-2019-fiesta}. Thus it is likely that by not having a standard validation split within the already standard train test splits will create reproducibility problems. However the parameters that were explored were justified as it was already shown by \citet{reimers-gurevych-2017-reporting} that it was important to report multiple runs of a neural based method. The C-values were tuned for one of the NP methods \citep{vo2015target} thus it seemed logical to investigate if this was important for the other NP method \citep{wang-etal-2017-tdparse}. Also scaling features was used within the codebase of one of the NP methods \citep{wang-etal-2017-tdparse}, but it was not stated to be used for the other NP method \citep{vo2015target}. Thus there are likely many parameters that are known and unknown that would affect reproducibility, but in this thesis only three key parameters were explored.  
    \item \textbf{How generalisable are existing methods within TDSA?} (\rqref{rq:generalisable})\newline
    There are two large limitations with this work. The first being the set of methods that were evaluated, which were in general Neural Pooling (NP) and LSTM methods. Even though they represent two different large sets of methods non-neural and neural, however there are a number of different non-neural and neural methods e.g. transformers \citep{vaswani2017attention} which have become very popular recently and unlike LSTM does not have a strong dependency/bias between its past encoded tokens. Further, both the NP and LSTM methods use the same type of input representation, the word embedding, where for the non-neural methods a bag of words input would be appropriate and could have created another strong and interesting baseline to compare too. The second limitation is that all datasets are in the same language, English, thus the findings are currently restricted to the English language only.
    \item \textbf{What is an appropriate empirical evaluation methodology for TDSA?} (\rqref{rq:measured})\newline
    The empirical evaluation methodology is limited to testing phenomena that can be automatically created but has not yet been tested for linguistic phenomena. There are many linguistic phenomena that would be of interest to explore such as those suggested by \citet{barnes-etal-2019-sentiment} that includes negation, amplifier, emoji, and many more.
\end{itemize}

\section{Future Work}
In this section, the future work will be stated that is based upon the findings within this thesis and the literature review. However, it is worth noting that there has been a very comprehensive survey on the future of sentiment analysis by \citet{poria2020beneath} that explores many worthwhile avenues for sentiment analysis in general.
\begin{itemize}
    \item As found within chapter \ref{chapter:reproducibility}, the generalisation experiments showed the neural methods under perform on small and un-balanced datasets. Thus testing different methods such as transfer learning from language models, which has been demonstrated to be successful in low resource settings \citep{howard-ruder-2018-universal}, and transfer learning from document sentiment analysis which has already been shown useful on un-balanced datasets \citep{he-etal-2018-exploiting} would be of interest.
    \item Through the error analysis studies in chapters \ref{chapter:methodology} and \ref{chapter:case_study_methodology} it was found that many methods did not perform well on target sentiment relationship modelling, measured through the \textit{DS} split and \textit{STAC Multi} metric. Thus finding a way to improve this would be of use.
    \item Also through the error analysis, within chapter \ref{chapter:methodology} and \ref{chapter:case_study_methodology}, Unknown Targets (\textit{UT}s) and especially Unknown Sentiment Known Targets (\textit{USKT}s) performed poorly in comparison to Known Sentiment Known Targets (KSKT). Again finding a way to improve on samples within the \textit{UT} and \textit{USKT} would be a potentially fruitful future direction, more so as these subsets of data tend to occur more within low resource settings as shown by chapter \ref{chapter:methodology}. A proposed approach to this problem would be to better model the target itself as done within \citet{he-etal-2018-effective}.
    \item The literature review highlighted, within section \ref{section:review_discourse}, the importance of discourse information within fine grained sentiment analysis. It would be of interest to study the importance of discourse information within TDSA. One area of discourse information that could be of use is co-reference resolution. This is motivated by the fact that \citet{kessler2009targeting} found that 14\% of targets are pronouns\footnote{The corpus this was performed on was an early version of JDPA corpus \citep{kessler2010icwsm}.}, thus if recent TDSA methods perform differently on these pronoun targets it could motivate the need for incorporating co-reference information into the TDSA method. Further, alternative discourse information could be of use too so that methods take into account more than the current sentence, as \citet{kessler2010icwsm} found that 9\% of sentiment expressions are not within the same sentence as the target they affect.
    \item \citet{poria2020beneath} indicates that commonsense information could help improve implicit and factual sentiment analysis and we agree with \citet{poria2020beneath} that this would be a fruitful avenue of future research. In the pursuit of this research the review on implicit and factual sentiment analysis, within section \ref{lit_review_further_related_topics_implicit_and_factual_Sentiment}, should be of use. %The literature review also highlighted, within section \ref{lit_review_further_related_topics_implicit_and_factual_Sentiment}, implicit and factual sentiment analysis within fine grained sentiment analysis. \citet{poria2020beneath} indicates that commonsense information could help improve implicit and factual sentiment analysis and we agree with \citet{poria2020beneath} that this would be a fruitful avenue of future research. In the pursuit of this research the review on implicit and factual sentiment analysis should be of use.
\end{itemize}

\label{section:conclusion_future_work}